{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc19703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_optimizer import Lamb\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_scheduler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from Bio import SeqIO\n",
    "import tqdm as tq\n",
    "import time\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "np.set_printoptions(threshold=2200)\n",
    "torch.set_printoptions(threshold=2200)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1fc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME, MODEL_PRINT_NAME = \"hunarbatra/CoVBERT\", \"CoVBERT\"\n",
    "VOC_NAMES = [\"Alpha\", \"Beta\", \"Delta\", \"Gamma\", \"Omicron\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc6bc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7093e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hunarbatra/CoVBERT were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at hunarbatra/CoVBERT and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model:  44897285\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(VOC_NAMES))\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Number of parameters in model: \", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a105ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for i, voc_name in enumerate(VOC_NAMES):\n",
    "    voc_df = pd.read_csv(f\"data/unique_{voc_name}_2k.csv\")\n",
    "    voc_df['label'] = i\n",
    "    df = pd.concat([df, voc_df])\n",
    "\n",
    "df = df.drop(['accession_id', 'date'], axis=1)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f160c581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVXX...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>TQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHA...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>TQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHA...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence  label\n",
       "0     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...      0\n",
       "1     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...      0\n",
       "2     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...      0\n",
       "3     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...      0\n",
       "4     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...      0\n",
       "...                                                 ...    ...\n",
       "9995  MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVXX...      4\n",
       "9996  TQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHA...      4\n",
       "9997  TQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHA...      4\n",
       "9998  MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...      4\n",
       "9999  MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...      4\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10273657",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['sequence'],\n",
    "                                                    df['label'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9664940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'sequence': X_train, 'label': y_train}).reset_index(drop=True)\n",
    "df_test = pd.DataFrame({'sequence': X_test, 'label': y_test}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e538ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327523c",
   "metadata": {},
   "source": [
    "# TOKENIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "388304fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569393e316da40f3a625ab871629bc04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f8813c09d7404b9427e2a60c428ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funtion to tokenize data\n",
    "def tokenize_dataset(dataset, tok, embedding_size):\n",
    "    return tok(\n",
    "        dataset['sequence'],\n",
    "        max_length=embedding_size,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\")\n",
    "    \n",
    "\n",
    "# Tokenize the dataset\n",
    "dataset_train = ds_train.map(tokenize_dataset, fn_kwargs={'tok': tok, 'embedding_size': EMBED_SIZE})\n",
    "dataset_test = ds_test.map(tokenize_dataset, fn_kwargs={'tok': tok, 'embedding_size': EMBED_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27d5345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sequence', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 8000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sequence', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train)\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7b6ccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 8000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Remove the sequence column because it will not be used in the model\n",
    "dataset_train = dataset_train.remove_columns(['sequence'])\n",
    "dataset_test = dataset_test.remove_columns(['sequence'])\n",
    "\n",
    "# Rename label to labels because the model expects the name labels\n",
    "dataset_train = dataset_train.rename_column(\"label\", \"labels\")\n",
    "dataset_test = dataset_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Change the format to PyTorch tensors\n",
    "dataset_train.set_format(\"torch\")\n",
    "dataset_test.set_format(\"torch\")\n",
    "\n",
    "# Take a look at the data\n",
    "print(dataset_train)\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce0511dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=16)\n",
    "eval_dataloader = DataLoader(dataset=dataset_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f50fe92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "tensor([ 0, 49, 42, 58, 42, 48, 58, 48, 48, 52, 48, 58, 55, 55, 53, 39, 58, 50,\n",
      "        48, 56, 56, 54, 56, 53, 48, 52, 52, 37, 61, 56, 50, 55, 42, 56, 54, 43,\n",
      "        58, 61, 61, 60, 40, 47, 58, 42, 54, 55, 55, 58, 48, 44, 55, 56, 53, 40,\n",
      "        48, 42, 48, 52, 42, 42, 55, 50, 58, 56, 59, 42, 44, 37, 45, 55, 43, 56,\n",
      "        50, 43, 56, 47, 54, 42, 40, 50, 52, 58, 48, 52, 42, 50, 40, 43, 58, 61,\n",
      "        42, 37, 55, 56, 41, 47, 55, 50, 45, 45, 54, 43, 59, 45, 42, 43, 56, 56,\n",
      "        48, 40, 55, 47, 56, 53, 55, 48, 48, 45, 58, 50, 50, 37, 56, 50, 58, 58,\n",
      "        45, 47, 58, 39, 41, 42, 53, 42, 39, 50, 40, 52, 42, 48, 43, 58, 61, 44,\n",
      "        47, 50, 50, 47, 55, 59, 49, 41, 55, 41, 42, 54, 58, 61, 55, 55, 37, 50,\n",
      "        50, 39, 56, 42, 41, 61, 58, 55, 53, 52, 42, 48, 49, 40, 48, 41, 43, 47,\n",
      "        53, 43, 50, 42, 47, 50, 48, 54, 41, 42, 58, 42, 47, 50, 45, 40, 43, 61,\n",
      "        42, 47, 45, 61, 55, 47, 44, 56, 52, 45, 50, 48, 58, 54, 40, 48, 52, 53,\n",
      "        43, 42, 55, 37, 48, 41, 52, 48, 58, 40, 48, 52, 45, 43, 45, 50, 45, 56,\n",
      "        54, 42, 53, 56, 48, 48, 37, 48, 44, 54, 55, 61, 48, 56, 52, 43, 40, 55,\n",
      "        55, 55, 43, 59, 56, 37, 43, 37, 37, 37, 61, 61, 58, 43, 61, 48, 53, 52,\n",
      "        54, 56, 42, 48, 48, 47, 61, 50, 41, 50, 43, 56, 45, 56, 40, 37, 58, 40,\n",
      "        39, 37, 48, 40, 52, 48, 55, 41, 56, 47, 39, 56, 48, 47, 55, 42, 56, 58,\n",
      "        41, 47, 43, 45, 61, 53, 56, 55, 50, 42, 54, 58, 53, 52, 56, 41, 55, 45,\n",
      "        58, 54, 42, 52, 50, 45, 56, 50, 48, 39, 52, 42, 43, 41, 58, 42, 50, 37,\n",
      "        56, 54, 42, 37, 55, 58, 61, 37, 59, 50, 54, 47, 54, 45, 55, 50, 39, 58,\n",
      "        37, 40, 61, 55, 58, 48, 61, 50, 55, 37, 55, 42, 55, 56, 42, 47, 39, 61,\n",
      "        43, 58, 55, 52, 56, 47, 48, 50, 40, 48, 39, 42, 56, 50, 58, 61, 37, 40,\n",
      "        55, 42, 58, 45, 54, 43, 40, 41, 58, 54, 53, 45, 37, 52, 43, 53, 56, 43,\n",
      "        47, 45, 37, 40, 61, 50, 61, 47, 48, 52, 40, 40, 42, 56, 43, 39, 58, 45,\n",
      "        37, 59, 50, 55, 50, 50, 48, 40, 55, 47, 58, 43, 43, 50, 61, 50, 61, 48,\n",
      "        61, 54, 48, 42, 54, 47, 55, 50, 48, 47, 52, 42, 41, 54, 40, 45, 55, 56,\n",
      "        41, 45, 61, 53, 37, 43, 55, 56, 52, 39, 50, 43, 58, 41, 43, 42, 50, 39,\n",
      "        61, 42, 52, 48, 53, 55, 61, 43, 42, 53, 52, 56, 61, 43, 58, 43, 61, 53,\n",
      "        52, 61, 54, 58, 58, 58, 48, 55, 42, 41, 48, 48, 44, 37, 52, 37, 56, 58,\n",
      "        39, 43, 52, 47, 47, 55, 56, 50, 48, 58, 47, 50, 47, 39, 58, 50, 42, 50,\n",
      "        42, 50, 43, 48, 56, 43, 56, 43, 58, 48, 56, 41, 55, 50, 47, 47, 42, 48,\n",
      "        52, 42, 53, 53, 42, 43, 54, 40, 45, 40, 40, 56, 56, 40, 37, 58, 54, 40,\n",
      "        52, 53, 56, 48, 41, 45, 48, 40, 45, 56, 52, 39, 55, 42, 43, 43, 58, 55,\n",
      "        58, 45, 56, 52, 43, 56, 50, 56, 55, 50, 53, 58, 37, 58, 48, 61, 53, 43,\n",
      "        58, 50, 39, 56, 41, 58, 52, 58, 37, 45, 44, 37, 40, 53, 48, 56, 52, 56,\n",
      "        59, 54, 58, 61, 55, 56, 43, 55, 50, 58, 42, 53, 56, 54, 37, 43, 39, 48,\n",
      "        45, 43, 37, 41, 44, 58, 50, 50, 55, 61, 41, 39, 40, 45, 52, 45, 43, 37,\n",
      "        43, 45, 39, 37, 55, 61, 53, 56, 53, 56, 50, 55, 44, 54, 54, 37, 54, 55,\n",
      "        58, 37, 55, 53, 55, 45, 45, 37, 61, 56, 49, 55, 48, 43, 37, 41, 50, 55,\n",
      "        58, 37, 61, 55, 50, 50, 55, 45, 37, 45, 52, 45, 50, 42, 56, 45, 55, 58,\n",
      "        56, 56, 41, 45, 48, 52, 58, 55, 49, 56, 47, 56, 55, 58, 40, 39, 56, 49,\n",
      "        61, 45, 39, 43, 40, 55, 56, 41, 39, 55, 50, 48, 48, 48, 53, 61, 43, 55,\n",
      "        42, 39, 56, 53, 48, 50, 54, 37, 48, 56, 43, 45, 37, 58, 41, 53, 40, 47,\n",
      "        50, 56, 53, 41, 58, 42, 37, 53, 58, 47, 53, 45, 61, 47, 56, 52, 52, 45,\n",
      "        47, 40, 42, 43, 43, 42, 50, 42, 55, 53, 45, 48, 52, 40, 52, 55, 47, 52,\n",
      "        55, 47, 54, 55, 42, 45, 41, 40, 48, 48, 42, 50, 47, 58, 56, 48, 37, 40,\n",
      "        37, 43, 42, 45, 47, 53, 61, 43, 40, 39, 48, 43, 40, 45, 37, 37, 54, 40,\n",
      "        48, 45, 39, 37, 53, 47, 42, 50, 43, 48, 56, 58, 48, 52, 52, 48, 48, 56,\n",
      "        40, 41, 49, 45, 37, 53, 61, 56, 55, 37, 48, 48, 37, 43, 56, 45, 56, 55,\n",
      "        43, 59, 56, 42, 43, 37, 43, 37, 37, 48, 53, 45, 52, 42, 37, 49, 53, 49,\n",
      "        37, 61, 54, 42, 50, 43, 45, 43, 58, 56, 53, 50, 58, 48, 61, 41, 50, 53,\n",
      "        47, 48, 45, 37, 50, 53, 42, 50, 55, 37, 45, 43, 47, 45, 53, 40, 55, 48,\n",
      "        55, 55, 56, 37, 55, 37, 48, 43, 47, 48, 53, 40, 58, 58, 50, 53, 50, 37,\n",
      "        53, 37, 48, 50, 56, 48, 58, 47, 53, 48, 55, 55, 50, 42, 43, 37, 45, 55,\n",
      "        55, 58, 48, 50, 40, 45, 48, 37, 54, 48, 40, 47, 58, 41, 37, 41, 58, 53,\n",
      "        45, 40, 54, 48, 45, 56, 43, 54, 48, 53, 55, 48, 53, 56, 61, 58, 56, 53,\n",
      "        53, 48, 45, 54, 37, 37, 41, 45, 54, 37, 55, 37, 50, 48, 37, 37, 56, 47,\n",
      "        49, 55, 41, 39, 58, 48, 43, 53, 55, 47, 54, 58, 40, 42, 39, 43, 47, 43,\n",
      "        61, 44, 48, 49, 55, 42, 52, 53, 55, 37, 52, 44, 43, 58, 58, 42, 48, 44,\n",
      "        58, 56, 61, 58, 52, 37, 53, 41, 47, 50, 42, 56, 56, 37, 52, 37, 45, 39,\n",
      "        44, 40, 43, 47, 37, 44, 42, 52, 54, 41, 43, 58, 42, 58, 55, 50, 43, 56,\n",
      "        44, 59, 42, 58, 56, 53, 54, 50, 42, 61, 41, 52, 53, 45, 45, 56, 56, 44,\n",
      "        50, 56, 42, 58, 55, 43, 50, 39, 40, 58, 58, 45, 43, 45, 58, 50, 50, 56,\n",
      "        58, 61, 40, 52, 48, 53, 52, 41, 48, 40, 55, 42, 47, 41, 41, 48, 40, 47,\n",
      "        61, 42, 47, 50, 44, 56, 55, 52, 40, 58, 40, 48, 43, 40, 45, 55, 43, 45,\n",
      "        50, 37, 55, 58, 58, 50, 45, 53, 47, 41, 45, 40, 54, 48, 50, 41, 58, 37,\n",
      "        47, 50, 48, 50, 41, 55, 48, 45, 40, 48, 53, 41, 48, 43, 47, 61, 41, 53,\n",
      "        61, 45, 47, 59, 52, 59, 61, 45, 59, 48, 43, 42, 45, 37, 43, 48, 45, 37,\n",
      "        45, 58, 49, 58, 56, 45, 49, 48, 39, 39, 49, 56, 55, 39, 42, 55, 39, 48,\n",
      "        47, 43, 39, 39, 55, 39, 43, 55, 39, 39, 47, 42, 40, 41, 40, 40, 55, 41,\n",
      "        52, 58, 48, 47, 43, 58, 47, 48, 44, 61, 56,  2,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "for batch in eval_dataloader:\n",
    "    item = batch[\"input_ids\"][0]\n",
    "    print(len(item))\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f66d2",
   "metadata": {},
   "source": [
    "# SET PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62b8dff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "# Number of training steps\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(params=model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "lr_scheduler = get_scheduler(name=\"linear\", \n",
    "                             optimizer=optimizer, \n",
    "                             num_warmup_steps=0, \n",
    "                             num_training_steps=num_training_steps)\n",
    "\n",
    "# Use GPU if it is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b42afe",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79b09050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.75 GiB (GPU 0; 47.50 GiB total capacity; 42.51 GiB already allocated; 1.11 GiB free; 44.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compute the model output for the batch\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get the logits\u001b[39;00m\n\u001b[1;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:1216\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1216\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1228\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:411\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    401\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:338\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    330\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 338\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch38/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:234\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    231\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    237\u001b[0m     query_length, key_length \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], key_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.75 GiB (GPU 0; 47.50 GiB total capacity; 42.51 GiB already allocated; 1.11 GiB free; 44.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Set the progress bar\n",
    "pb_train = tq.tqdm(range(num_training_steps))\n",
    "\n",
    "# Tells the model that we are training the model\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()\n",
    "# Loop through the epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop through the batches\n",
    "    for batch in train_dataloader:\n",
    "        # Get the batch\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # Compute the model output for the batch\n",
    "        outputs = model(**batch)\n",
    "        # Get the logits\n",
    "        logits = outputs.logits\n",
    "        # Get the predicted labels for the batch\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        # Loss computed by the model\n",
    "        loss = outputs.loss\n",
    "        # backpropagates the error to calculate gradients\n",
    "        loss.backward()\n",
    "        # Update the model weights\n",
    "        optimizer.step()\n",
    "        # Learning rate scheduler\n",
    "        lr_scheduler.step()\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Update the progress bar\n",
    "        pb_train.update(1)   \n",
    "        pb_train.set_description(\"Loss={:.4f} Predictions={}\".format(loss.item(), predictions.tolist()[0:5]))\n",
    "    print('Epoch {}/{} Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "formatted_time = str(timedelta(seconds=elapsed_time))\n",
    "\n",
    "print(f\"Elapsed time: {formatted_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9326c51",
   "metadata": {},
   "source": [
    "# EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57285407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▎         | 2/80 [00:00<00:11,  6.93it/s]\u001b[A\n",
      "  4%|▍         | 3/80 [00:00<00:15,  4.99it/s]\u001b[A\n",
      "  5%|▌         | 4/80 [00:00<00:17,  4.35it/s]\u001b[A\n",
      "  6%|▋         | 5/80 [00:01<00:18,  4.02it/s]\u001b[A\n",
      "  8%|▊         | 6/80 [00:01<00:19,  3.85it/s]\u001b[A\n",
      "  9%|▉         | 7/80 [00:01<00:19,  3.75it/s]\u001b[A\n",
      " 10%|█         | 8/80 [00:01<00:19,  3.69it/s]\u001b[A\n",
      " 11%|█▏        | 9/80 [00:02<00:19,  3.64it/s]\u001b[A\n",
      " 12%|█▎        | 10/80 [00:02<00:19,  3.61it/s]\u001b[A\n",
      " 14%|█▍        | 11/80 [00:02<00:19,  3.58it/s]\u001b[A\n",
      " 15%|█▌        | 12/80 [00:03<00:19,  3.57it/s]\u001b[A\n",
      " 16%|█▋        | 13/80 [00:03<00:18,  3.56it/s]\u001b[A\n",
      " 18%|█▊        | 14/80 [00:03<00:18,  3.55it/s]\u001b[A\n",
      " 19%|█▉        | 15/80 [00:03<00:18,  3.54it/s]\u001b[A\n",
      " 20%|██        | 16/80 [00:04<00:18,  3.54it/s]\u001b[A\n",
      " 21%|██▏       | 17/80 [00:04<00:17,  3.55it/s]\u001b[A\n",
      " 22%|██▎       | 18/80 [00:04<00:17,  3.53it/s]\u001b[A\n",
      " 24%|██▍       | 19/80 [00:05<00:17,  3.52it/s]\u001b[A\n",
      " 25%|██▌       | 20/80 [00:05<00:17,  3.52it/s]\u001b[A\n",
      " 26%|██▋       | 21/80 [00:05<00:16,  3.52it/s]\u001b[A\n",
      " 28%|██▊       | 22/80 [00:05<00:16,  3.52it/s]\u001b[A\n",
      " 29%|██▉       | 23/80 [00:06<00:16,  3.51it/s]\u001b[A\n",
      " 30%|███       | 24/80 [00:06<00:15,  3.51it/s]\u001b[A\n",
      " 31%|███▏      | 25/80 [00:06<00:15,  3.49it/s]\u001b[A\n",
      " 32%|███▎      | 26/80 [00:07<00:15,  3.51it/s]\u001b[A\n",
      " 34%|███▍      | 27/80 [00:07<00:15,  3.50it/s]\u001b[A\n",
      " 35%|███▌      | 28/80 [00:07<00:14,  3.50it/s]\u001b[A\n",
      " 36%|███▋      | 29/80 [00:07<00:14,  3.49it/s]\u001b[A\n",
      " 38%|███▊      | 30/80 [00:08<00:14,  3.47it/s]\u001b[A\n",
      " 39%|███▉      | 31/80 [00:08<00:14,  3.48it/s]\u001b[A\n",
      " 40%|████      | 32/80 [00:08<00:13,  3.47it/s]\u001b[A\n",
      " 41%|████▏     | 33/80 [00:09<00:13,  3.48it/s]\u001b[A\n",
      " 42%|████▎     | 34/80 [00:09<00:13,  3.48it/s]\u001b[A\n",
      " 44%|████▍     | 35/80 [00:09<00:12,  3.48it/s]\u001b[A\n",
      " 45%|████▌     | 36/80 [00:09<00:12,  3.47it/s]\u001b[A\n",
      " 46%|████▋     | 37/80 [00:10<00:12,  3.48it/s]\u001b[A\n",
      " 48%|████▊     | 38/80 [00:10<00:12,  3.48it/s]\u001b[A\n",
      " 49%|████▉     | 39/80 [00:10<00:11,  3.48it/s]\u001b[A\n",
      " 50%|█████     | 40/80 [00:11<00:11,  3.47it/s]\u001b[A\n",
      " 51%|█████▏    | 41/80 [00:11<00:11,  3.46it/s]\u001b[A\n",
      " 52%|█████▎    | 42/80 [00:11<00:10,  3.46it/s]\u001b[A\n",
      " 54%|█████▍    | 43/80 [00:11<00:10,  3.46it/s]\u001b[A\n",
      " 55%|█████▌    | 44/80 [00:12<00:10,  3.46it/s]\u001b[A\n",
      " 56%|█████▋    | 45/80 [00:12<00:10,  3.46it/s]\u001b[A\n",
      " 57%|█████▊    | 46/80 [00:12<00:09,  3.45it/s]\u001b[A\n",
      " 59%|█████▉    | 47/80 [00:13<00:09,  3.45it/s]\u001b[A\n",
      " 60%|██████    | 48/80 [00:13<00:09,  3.45it/s]\u001b[A\n",
      " 61%|██████▏   | 49/80 [00:13<00:09,  3.44it/s]\u001b[A\n",
      " 62%|██████▎   | 50/80 [00:14<00:08,  3.44it/s]\u001b[A\n",
      " 64%|██████▍   | 51/80 [00:14<00:08,  3.44it/s]\u001b[A\n",
      " 65%|██████▌   | 52/80 [00:14<00:08,  3.42it/s]\u001b[A\n",
      " 66%|██████▋   | 53/80 [00:14<00:07,  3.43it/s]\u001b[A\n",
      " 68%|██████▊   | 54/80 [00:15<00:07,  3.43it/s]\u001b[A\n",
      " 69%|██████▉   | 55/80 [00:15<00:07,  3.42it/s]\u001b[A\n",
      " 70%|███████   | 56/80 [00:15<00:07,  3.42it/s]\u001b[A\n",
      " 71%|███████▏  | 57/80 [00:16<00:06,  3.42it/s]\u001b[A\n",
      " 72%|███████▎  | 58/80 [00:16<00:06,  3.42it/s]\u001b[A\n",
      " 74%|███████▍  | 59/80 [00:16<00:06,  3.41it/s]\u001b[A\n",
      " 75%|███████▌  | 60/80 [00:16<00:05,  3.41it/s]\u001b[A\n",
      " 76%|███████▋  | 61/80 [00:17<00:05,  3.42it/s]\u001b[A\n",
      " 78%|███████▊  | 62/80 [00:17<00:05,  3.42it/s]\u001b[A\n",
      " 79%|███████▉  | 63/80 [00:17<00:04,  3.41it/s]\u001b[A\n",
      " 80%|████████  | 64/80 [00:18<00:04,  3.41it/s]\u001b[A\n",
      " 81%|████████▏ | 65/80 [00:18<00:04,  3.41it/s]\u001b[A\n",
      " 82%|████████▎ | 66/80 [00:18<00:04,  3.38it/s]\u001b[A\n",
      " 84%|████████▍ | 67/80 [00:19<00:03,  3.40it/s]\u001b[A\n",
      " 85%|████████▌ | 68/80 [00:19<00:03,  3.39it/s]\u001b[A\n",
      " 86%|████████▋ | 69/80 [00:19<00:03,  3.39it/s]\u001b[A\n",
      " 88%|████████▊ | 70/80 [00:19<00:02,  3.40it/s]\u001b[A\n",
      " 89%|████████▉ | 71/80 [00:20<00:02,  3.39it/s]\u001b[A\n",
      " 90%|█████████ | 72/80 [00:20<00:02,  3.39it/s]\u001b[A\n",
      " 91%|█████████▏| 73/80 [00:20<00:02,  3.39it/s]\u001b[A\n",
      " 92%|█████████▎| 74/80 [00:21<00:01,  3.39it/s]\u001b[A\n",
      " 94%|█████████▍| 75/80 [00:21<00:01,  3.38it/s]\u001b[A\n",
      " 95%|█████████▌| 76/80 [00:21<00:01,  3.39it/s]\u001b[A\n",
      " 96%|█████████▋| 77/80 [00:21<00:00,  3.39it/s]\u001b[A\n",
      " 98%|█████████▊| 78/80 [00:22<00:00,  3.38it/s]\u001b[A\n",
      " 99%|█████████▉| 79/80 [00:22<00:00,  3.38it/s]\u001b[A\n",
      "100%|██████████| 80/80 [00:22<00:00,  3.38it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# Tells the model that we are evaluting the model performance\n",
    "model.eval()\n",
    "\n",
    "#  A list for all logits\n",
    "logits_all = []\n",
    "\n",
    "# A list for all predicted probabilities\n",
    "predicted_prob_all = []\n",
    "\n",
    "# A list for all predicted labels\n",
    "predictions_all = []\n",
    "\n",
    "# A list for all actual labels\n",
    "labels_all = []\n",
    "\n",
    "\n",
    "# Set the progress bar\n",
    "pb_eval = tq.tqdm(range(len(eval_dataloader)))\n",
    "\n",
    "# Loop through the batches in the evaluation dataloader\n",
    "for batch in eval_dataloader:\n",
    "    # Get the batch\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # Disable the gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Compute the model output\n",
    "        outputs = model(**batch)\n",
    "    # Get the logits\n",
    "    logits = outputs.logits\n",
    "    # Append the logits batch to the list\n",
    "    logits_all.append(logits) \n",
    "    # Get the predicted probabilities for the batch\n",
    "    predicted_prob = torch.softmax(logits, dim=1)\n",
    "    # Append the predicted probabilities for the batch to all the predicted probabilities\n",
    "    predicted_prob_all.append(predicted_prob)\n",
    "    # Get the predicted labels for the batch\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    # Append the predicted labels for the batch to all the predictions\n",
    "    predictions_all.append(predictions)\n",
    "    # Append the actual labels for the batch to all labels\n",
    "    labels_all.append(batch[\"labels\"])\n",
    "    pb_eval.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf72b7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9186    0.9025    0.9105       400\n",
      "           1     0.9274    0.8300    0.8760       400\n",
      "           2     0.9831    0.8725    0.9245       400\n",
      "           3     0.6411    0.9600    0.7688       400\n",
      "           4     0.9661    0.7125    0.8201       400\n",
      "\n",
      "    accuracy                         0.8555      2000\n",
      "   macro avg     0.8872    0.8555    0.8600      2000\n",
      "weighted avg     0.8872    0.8555    0.8600      2000\n",
      "\n",
      "CONFUSION = [[361, 11, 1, 25, 2], [0, 332, 4, 64, 0], [4, 4, 349, 39, 4], [6, 5, 1, 384, 4], [22, 6, 0, 87, 285]]\n"
     ]
    }
   ],
   "source": [
    "all_preds = torch.cat(predictions_all, dim=0).cpu().numpy()\n",
    "all_labels = torch.cat(labels_all, dim=0).cpu().numpy()\n",
    "\n",
    "cr = classification_report(all_labels, all_preds, labels=[0,1,2,3,4], digits=4)\n",
    "print(cr)\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1,2,3,4])\n",
    "print('CONFUSION = {}'.format(cm.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c5eb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f'results/{MODEL_PRINT_NAME}_{EMBED_SIZE}_all_preds.txt', all_preds, fmt='%d')\n",
    "np.savetxt(f'results/{MODEL_PRINT_NAME}_{EMBED_SIZE}_all_labels.txt', all_labels, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b090703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdsAAAGMCAYAAACS8Qh7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAABf2UlEQVR4nO3dd3gU1dfA8e9J6L1KV0A6SFdQQATsBcSKYgMF7D97AxVFBGwodoqCBQG7KFgQsbyISBMpSu8dAgKhhvP+cSdhs6RsEnZnszkfnnnYvTM7c3ayu2funTtzRVUxxhhjTPjE+R2AMcYYE+ss2RpjjDFhZsnWGGOMCTNLtsYYY0yYWbI1xhhjwsySrTHGGBNmlmxzKRHpLyILom1dIWyruoioiLQMKGsjIvNF5KCITEtrmTDFcpOI7AnnNiLteL8n7+/xWg7X0V5ElohI/PGKKzcTkZkicrnfcZjIsmQbJURktJdgRqUxb4g37+uA4heA9pGL8LhZC1QC5gWUvQL8BZwMXJbOMjni7b8rgorHAzWP1zZCiKGCiOwXkTUiklu+e5cBj+ZwHc8DA1U1KblARAqIyIMiMldEEkVkh4jMEJE+IlIwsxWKSHPvb9ounfnjRWS697i/t2zytFNEfhKR1kGvWRW0XPI02JtfPah8lxfzJd78aem8Pnla5W1qADA4F30GzHFgf+zosha4SkSKJheISD7gBmBN4IKqukdVt0c4vhxT1SRV3aSqhwOKawFTVXWtqu5IZ5lwxLJPVbeEcxtBbgQmAvuB8yK43Wzz/h67s/t6ETkDqAdMCCgrAHwH9AXeBdoALYCXgB7A6SHENQd3MNYzjW2WBS4FRgYU/4s7gKvkbW8TMFlECgW9/OmA5ZKnZ4KWOd8rbwXMBD4VkUa4A5Pk1zT0lr08oOxUr2wSUBy4ILP3aWKHJdvoMh9YClwVUHYR7sd5WuCCwU2/InKKiPwoIv+JyB4R+UtEOgTMryciX3lH43tE5HcROSWtIETkVBH5XkS2eev7TUROD1qmj9c0uN9b7jvvwCDDWAKbiJMfAyWBd7zym9Jpak43/sziDahRfBxYw5A0mly997VMXJP2MhHpFTRfRaS3iHwsIntFZIWIXJfWfkxDT+A94H3g5jT2e6brFpHBIvKviOzzamLPpZEwkpetLiJHJKg5XkR6efuqgIjkF5FhIrJBRA6IyNrkmpy3bKpmZBG5TFyT/z5xtdGfRaRCBu/5WuBHVd0XUHYPrlXmbFUdpqpzVXWlqk4AzgDmeNsqKCIvi8hm73M2Q0TaBqxnJHCliBQL2uZ1wAFcy0Wyw94B3CZVXQgMBEoBNYJeuztgueQpuFl+u1f+D+6AIT/QwTsw2aSqm4Dkg7gdAevZCu6AE5dwr8lgv5kYY8k2+owi9dF6T9zRf2b31RwLbAROA5oC/XFJGhGpDPzmreMcoDnwOpDeObTiuITQzlvfPGCSV2PA+/F+HXgKqAt0Ar4NJZYgyc3Fibgf4Eqk/oEkxPgzjJejNYpepK5hBG+nK/Aa8DLQCNe8/YZ4zYQBngC+BJp48b4jIiemtc6AdbcDyuL20wfAxSJSPo1FM1v3Xtxnoj5wO9AN94N/DFVdBfzAsbW/nsD7qnoQuBvo6q2nNnA1rhaY1nuoCIwDxnjbPxO33zPSDpgVVNYdmKKqweWo6hFV/c97+pwXT0+gGfA38K2IVPLmf4j7DFwdtJqbgfGqujed91EIuB6XEFdlEn+6RCQ/7jMFcCiLL59J7jwNZLJLVW2KggkYDXwNlAb24X74KuKO0E9Mnh+wfH9gQcDz/4Ab01n3QGA1UCCd+anWlcZ8wSXP67znlwG7gOLpLJ9RLNVxSbNlQNke4Kb0lsks/szi9coUuCJouZuAPQHP/w94J42/y29B6xkU8Dwf7mDhukxiGg28FvD8F+CBoGWyvG7gVmBZBu/pCiABKOQ9r+9tp5H3fBjwIyDprH9acty4gxwFTsrC53on0COoLBF4JZPXFQUOAjcElMUDy4FnAso+AKYHPD/Vi7FV0Oc7yfuc7QGOANtxtdHAba7Cfd/2BE0XB30uE73yJO/5CqBM0LrKefPOSuf9dfbiyBfqvrQpd09Ws40yqpoAfI47mr8RmKaqazJ+FeDOd40Ukaki0ldE6gXMa4ZLGAdDiUFEThCRt71m4l3AbuAEXNIHV1taDawUkQ9F5EYRKR5iLNmRYfwhxBuq+riEG+g3oEFQ2fzkB+rOK2/1tpcmESkBXEnqWmCaTcmZrVtErvCayTd5TeBDyfh9folLWpd5z3sCM1U1+RTEaFzrwxIReV1ELpL0O+78BUwBFojIpyJyWzq180CFObZVQzJ5DbjOcvkJ+Huoa379ndR/j5HA6QGfsZ64A8c/gta3HPc+m+LOD48AvhSR5kHLvRSwXPL0U9Ay1+I+k51xp316quqOEN5ToH24/ZDmKQATeyzZRqd3cJ2ienqPM6Wq/XE/Ql/gznvNF5FjOo+EaAyuhnCvt66mwDqggLet3bhazlW4jluPAv94zb3HO5Ycx3scBDfhBzcZKhl/l64FigD/JyKHReQw8CZQT0TahLpucb1nx+E6F12C+8Hvh0tKaQeuegh3nrinuHPq1+NOVSTPn4OrsT3qbWcM8ENaCddLdud603zcwcJSEWmSwXvfhmutCbQEd2CTXYF/j5+BZbj3Vxh3HvSYHv3AQVVd5k1zVfURYAPuMxNoe8ByyVNwc/Q6VV2qqt/gmpEniEi5LL6HMsB+PfZ8sIlRlmyj04+42kg5XMIKifcDMExVL8L94NzizZoLtBXXCzQUbYFXVfUbdZ1JduPOdQZu67CqTlXVR4HGuGa/i0OIJTsyiz/TeHFJLLPrPBfjeqoGr3tRFuMNdjPuXHDToOkb0q7dpqcNsF5VB6jqn6q6FDgphNeNBDrgzvEWxyXsFKq6W1U/UdXbcB3yOuJ6iB9Dnd9V9SncAc4Gjj1nGmgux7YMjAXODu64BSAicV5LwHLcd6BNwLx4XE/llL+HqipHD06vwdWkMzuPnCwJdxCUbar6sxfPE1l8aSO8jmAmb7BkG4W8H5DGQA1VPZDZ8iJS2GsCPMvrgdqK1EniDaAY7gj8VBGpJSLXiEjTdFa5BLhORBqIyKm4H+eUJlwRuVhE/icizUTkJFzNrTiwOIRYsiOz+DOM17MK6CQiFUUkuKaV7HngehG5Q0Rqi8hduM48z2U3cBFpDLQERqjqgsAJlxSuCmqCz8gSoIqIdBeRmiJyGyH0aFXVf3HN4c8Dn+jRDkiIyH3evqwvIrVwf8v/cC0Dwe+ltYj08/4GJ+KaUauR8d/2O9zfP9DLXjw/iMjdItJURGqIyGVeeXOvNvkmMERELhSR+t7zCrjPQ6AxuAPTF4AvNO1L4vJ5f/uK3t+2H+4g4Mug5YoHLJc8lczg/QG8CPQWkWqZLBeoHak7FZpY5/dJY5vcRFAHqMzmE9CpCddcOpajHTw2AMOBEgHLN8RdbrAHV/ObztFOMinr8p43Af7AnVdajmt6XAD09+a3xZ3H2u4tswCvE0xmsZCNDlIhxJ9hvN4yl+DOrx0CVnllNxHQmcgruxXXLHnI+79X0Py0OlqtIqizU8C8YcCSdOYVxXW26R3quoFBuPO4e4DPgNvwjs/Se09e+Q3e+s8MKu+Fq2HtxiXZn4EzAuZP42gHqfrAZGCz97ddBjyUyee6tPceGwaVFwQexp0H3ofrxDUD6IPXEc5b5uWA7c0A2qazna+893dOGvP6e/OSp724ZvBb09jXmsb0QXqfS69cgH+A4QFl6XaQAqrgDgar+vV7Y1PkJ/H++MaYGCYiDwM3q2odH7Y9GCivqllpMo9ZIvI8UFJVe/sdi4kca0Y2JoaJSDERaQj8D3fdsB+eBVaI3Rs52Rbgcb+DMJFlNVtjYpiIjMad1/0KuEbDfAtMY0zaLNkaY4wxYWbNyMYYY0yYWbI1xhhjwiyf3wFkV+EOA6z9OwNbJqd5b3oD5M9nx5jp2b3fTulmpFjBXPuTGXaF84d0G87srbvZnTn6vd8397WwxRYq++QYY4yJbunerjv3sGRrjDEmuonvFdMcs2RrjDEmusVAzTb3vwNjjDEmylnN1hhjTHSzZmRjjDEmzGKgGdmSrTHGmOgWAzXb3H+4YIwxxkQ5q9kaY4yJbtaMbIwxxoRZDDQjW7I1xhgT3axma4wxxoRZDNRsc//hgjHGGBPlrGZrjDEmulkzsjHGGBNmMdCMHPFkKyIdgGuAE4ECgfNUtWOk4zHGGBPlYqBmG9F3ICI3AZOB4sBZwFagNNAcWBTJWIwxxuQSEpezKQpEOooHgDtV9RrgEPCoqjYDPgD2RDgWY4wxJiIinWxrAlO8xweAYt7j14CbIhyLMcaY3CBOcjZFgUgn2+24JmSA9UAj73FZoHCEYzHGGJMbxEAzcqQ7SP0KnAv8DUwAhonIOUAn4IcIx2KMMSY3sN7IWXYnUMh7PAg4DLTBJd5nIhyLMcYYExERTbaquiPg8RFgSCS3b4wxJheKkqbgnPDlphYiUhk4gaBzxqo6x494jDHGRDFrRs4aEUm+zKceELz3FIiPZDzGGGNygTDXbEWkEPALUBCXFz9R1SdFZDTQHtjlLXqTqs4TEQFeAS4EEr3yDCuLka7ZDgfWAr2ADbgEa4wxxqQv/DXbA0BHVd0jIvmB30RksjfvQVX9JGj5C4Da3tQKeNP7P12RTrYNgGaquiTC282yPpe25OaLm3NSxVIALF61lcEf/Mq3M5alLFOrahme6dWJ9s2rUyBfPP+u2UaPgV/w75ptAPS8uBlXdWxEk9oVKVWsEHW7DWPN5l1pbS7XmzP7T94f8y7/LFrI1q1bePLpZ7mkS9eU+VOnfM9nn0zgn38WsTMhgbdGjqHlqaf5GLG/Zs/6kzHvjmLRooVs3bKFp58ZRJeul/kdVsS9/84Ifv7pB9asXkWB/AVocEpjbr3zXmrWqp2yzMAnH2Py11+mel2DRo0ZPuajSIfru1Ej3ubHKd+zetVKChQowCmNm3L3PfdRq3Ydv0PL1VRVOXpjpfzelFFlsAvwnve6GSJSSkQqqerG9F4Q6bPOfwMVI7zNbFm/9T/6Df+R03uPoM2tI5k2dxUTBlxFo5onAHBSxVJMffUmVm1K4IL73qdFz7d46p1p7N13MGUdRQrmZ8qsFQwc/bNfbyNiEhMTqVWrNvc//BgFCxU6Zv6+ffto3LQZ997/sA/RRZ/ExERq1a7Dw4/0pVAa+yuvmDt7Jl2vvIa33vmQV956h/j4fNxz+838t2tnquVatjqdL7+bljK9MOxNfwL22aw/Z3JVt2sZ88E4ho8aQ3x8PH1u6cGuoP0VcyJwna2IxIvIPGAL8IOq/uHNGigi80VkqIgU9Mqq4Fppk63zytIV9pqtiJQJePoY8JyI9MMl3kOBywb2Vvbb1/+XuvLdf9RP9OrcglYNq7JgxRaeuqUDP85awSNvTklZZtXGnale89qnMwFoXqdS2OP1W9t27Wnbrj0ATz3+2DHzL7qkCwA7ExIiGle0andme9qd6fbX430f9Tka/7z0+ohUzx8fMIjz27dm/l9zaXtmh5TyAvkLULZc+UiHF3XeHD4q1fOBg5+jbeuWzJs7h/ZnxfA4LjlsRhaR3kDvgKLhqjo8cBlVTQKaikgp4HMRaQQ8CmzCDZozHHgYeDo7MUSiGXkbqavjAnyfRlnUdpCKixMub9+AYoULMGPBOkTgwtNr88LY6Xw55Bqa1anEmk27eHnC73zyk42nYEx2Je5N5MiRIxQvXiJV+fx5c7j47HYUK16cZs1b0vuO/1G6TFmfoowee/fu5ciRI5QoUSLzhXOzHHaQ8hLr8EwXdMvuFJGfgPNV9QWv+ICIvIu7vz+4OyBWC3hZVa8sXZFIth0yXyQ6NaxxAtNe70GhAvnYs+8gVz8xgYUrt1ChdFGKFynIQ93b8PS703h8+FTOal6dd/t2Zc++g6nO6xpjQvfKC4OoXbcejRo3TSlrdUZb2nc8m0qVq7Jx43pGvDGMu2/tyagPPqZAgQLprywPeG7wQOrWq0/jJs38DiW8wtxBSkTKA4e8RFsYOAcYknwe1ut9fCmwwHvJV8CdIjIO1zFqV0bnayECyVZVj9sJy8CmgHx1OpOvcsvjteo0LVm7jVa3DKdksYJ0PbMBIx7pwnn3vEfC7n0AfD19CcM+ds3685dvpnndytx66amWbI3JhldfGsL8eXN4Y9T7xMcfbeQ6+7wLUx6fXLsO9eo35PKLzuH3336mfcdz/Ag1Krzw3CDmzZnNu+99lGp/mWypBIwRkXhcX6YJqvq1iEz1ErEA84BbveUn4S77WYa79KdHZhvwY/D4SsBtuJ7JAIuBN1V1Q2avDWwKKNxhQNgvGzp0+AgrNrhzjHOXbKJFvUrcdWUr7h46iUOHk1i8amuq5f9ZvY0rOzYMd1jGxJxhLw7mx+8mM+ztd6lStVqGy5YrfwInVKjA2jWrIxRd9Hl+yLN8N3kSI94ZQ9VqGe+vmBDm62xVdT5wTPOAqqZ5ItzrhXxHVrYR6cHjzwGWA1fjjgYSgSuBZSJybiRjyY44EQrmz8ehw0eY/c8G6lRLfc6odtUyrNm005/gjMmlXn5+EFO+m8Qrb7/DSTVqZrr8zoQEtm7ZnGc7TA0Z9AzfTvqG4aPGUKPmyX6HExk26k+WDQNGAv/zjgwAEJFXcHfjqB/heNI1oFdHvp2xlLVb/qN4kYJc3akRZzatTtdH3bV9L437nQ+evJz/+3sN0+ason2z6lzZsSFXPT4hZR0VShelQpli1PaScv3q5SlVrBBrt+wiYfd+X95XuCQm7mXtmjUAHNEjbNq4gX//WUzJkiWpWKkyu3btZNPGjezevRuAdWtXU7x4ccqWK0e5PPijmbh3L2u8/aV6hI0bN/DPYre/KlWu7HN0kfPi4AF8N2kig14YRvHiJdi+zbUWFS5ShCJFipKYuJd33n6DszqdQ9ly5dm4YT1vv/YypcuUpX2Hs32OPvKefeYpvpn4JUOHvU6JkiXY5u2vIt7+ilkxcLtGCch54d+YyD6gSfBNLUSkDjBPVYuEuq5wNyMPf7gz7ZueRIUyxdi19wALVmxm6PjfmfLnipRlrjuvMQ91b0vVE0qwbN0OXhj7f0yYujBlft8bz6TfTe2PWXevwV/ywXfzwxk+Wyb3Dev6g836cya33nLjMeUXd76U/gMGMfHLz3nqiWMvCep16x30ue3OSISYIn8+/490/5z5B7f0uOGY8s5dujLg2cE+ROTs3n84ottr2yLt0y49et/OzX3u4MD+/Tx6/10s+fcf9uz+j7LlytO85WnccttdVKgY+UvqihX05XbyKZo2qptmeZ/b7uS2O+6KcDSpFc5/zC14j9+6O7+Zo9/7fV/d5nu2jnSy/RV4WVU/DSq/HLhPVduEuq5InLPNzSKdbHOTaEi20SrSyTa38TvZRrOwJtsub+cs2X7Zx/dkG+lPzhvAUBGpDczwylrjOkw9IiLNkxe0EYCMMcYAMdGMHOlk+6H3/7MZzIMovsGFMcaYCIuSTk45EelkWyPC2zPGGJPbWc02a1Q1714YZ4wxJs+KxEAEIY8bpqqfhTMWY4wxuY9YzTYkwYPupsfO0xpjjDmGJdsQqGpIZ7a9u0sZY4wxqeX+XBv5eyMHEpEquBs49wCqYzVbY4wxMSji/alFJF5ELhORScAqoCvwNlAr0rEYY4yJfiKSoykaRKxmKyJ1gVuAG4C9wFjcmIHXq6qNuG6MMSZN0ZIwcyIiNVvvNo0zgNLAVapaU1X7RWLbxhhjcjer2YbudOB1YLiqLsxsYWOMMSZZtCTMnIjUOdtTcYn9NxGZKyL3ikjFCG3bGGOM8VVEkq2qzlXVO4BKwEtAZ2Ctt/2LRKR0JOIwxhiTC0kOpygQ0d7IqrpfVd9X1Q64geKfB+4FNonI5EjGYowxJneIhXO2vg2loKrLVPURoBpwFXDQr1iMMcZEr1hItr6PhKyqScCX3mSMMcakEi0JMydy/yCBxhhjTJTzvWZrjDHGZCQWaraWbI0xxkS33J9rLdkaY4yJbrFQs7VztsYYY0yYWc3WGGNMVIuFmq0lW2OMMVHNkq0xxhgTbrk/11qyNcYYE91ioWZrHaSMMcaYMMu1NduEHx73O4SoVrbbu36HELW2fnST3yFErW27D/gdQlQrXijX/mTmarFQs7VPjjHGmKhmydYYY4wJs1hItnbO1hhjTHQL8+DxIlJIRGaKyF8islBEnvLKa4jIHyKyTETGi0gBr7yg93yZN796ZtuwZGuMMSavOwB0VNUmQFPgfBFpDQwBhqpqLSABuNlb/mYgwSsf6i2XIUu2xhhjolq4B49XZ4/3NL83KdAR+MQrHwNc6j3u4j3Hm99JMtmQJVtjjDFRLafJVkR6i8isgKl3GtuIF5F5wBbgB2A5sFNVD3uLrAOqeI+rAGsBvPm7gLIZvQfrIGWMMSaq5bSDlKoOB4ZnskwS0FRESgGfA/VytNEgVrM1xhhjPKq6E/gJOB0oJSLJldKqwHrv8XqgGoA3vySwPaP1WrI1xhgT3cLfG7m8V6NFRAoD5wCLcUn3Cm+xG4Evvcdfec/x5k9VVc1oG9aMbIwxJqpF4DrbSsAYEYnHVUInqOrXIrIIGCcizwBzgVHe8qOA90VkGbAD6JbZBizZGmOMiWrhTraqOh9olkb5CuC0NMr3A1dmZRuWbI0xxkQ1u4OUMcYYYzJlNVtjjDFRLRZqtpZsjTHGRLfcn2st2RpjjIluVrM1xhhjwiwWkq11kDLGGGPCzGq2xhhjoloMVGz9T7YiUhEoEFimqmt8CscYY0yUiYVmZF+SrYiUBIYBVxGUaD3xkY3IGGNMtIqBXOvbOdsXgCa4gXj3A9cCD+LGC7zap5iMMcaYsPCrGfkC4BpV/VVEkoDZqjpeRDYCfXAj3xtjjDEx0YzsV822FLDaexw4wv3vwBl+BGSMMSY6ieRsigZ+JdvlQE3v8WKgm7hDl8twwxUZY4wxAMTFSY6maOBXM/JooDEwDRgMfA3ciUv+//MpJmOMMVEoWmqnOeFLslXVoQGPp4pIPaAlsFRV//Yjpuwa/9GHjH53FNu2buXkWrV56JHHaN6ipd9hhVXv8+tx8zl1ObF8MQAWr93JkE//4rs56wB4vFszup5enapli3Lw8BH+Wrmdp8fN5Y9/twBQulgB+l7VjI5NKnNiuWJs332AybPX8vRHc9ix54Bv78svo0a8zWuvDOXqa67lkb5P+B1OxO3YvpX3h7/K7Bm/sS8xkQqVq3DrvY/RqGmLY5Z948Vn+H7iZ9x06z1c2u0GH6KNDnnxdye386UZWURuEJGCyc9VdY2qfgb8KyK55hv07eRJPDf4WW7pdSvjP/mCJk2bcXufXmzcsMHv0MJq/fZEHv9gFm0e+op2D0/k5wUbGf9QJxqdVBqApet3cd/IGZx23xec028Sqzbv4Yu+53BCyUIAVCpdhMpli9Lv/Vmcdv8X3DzsZ9o0qMDoe9v7+bZ8Mf+veXz2yQRq16nrdyi+2LN7N4/e2RNVpd/gYbz23qf0uvshSpYufcyy06dNYenihZQpV96HSKNHXvzdEZEcTdHAr3O27wIl0ygv7s3LFd4f8y6du3Tl8iuvoubJJ/No38cpX748E8Z/5HdoYfXNn2v4fu56VmzazbKN//HUR3PYve8Qp9U5AYBxv65g2t8bWbVlD4vX7eSRMTMpUaQAjauXAWDR2p1c+/xUJs1ay4pNu/lt0Wb6vTeLDqdUpnjh/H6+tYjavXs3fR9+kP4DBlKiRAm/w/HF5+NGU7psOe55bAB16jeiQqUqNGnRimon1Uy13JZNGxj52vPc9/hA4uN9vxePr/Li7451kMo+ATSN8hNxvZOj3qGDB1m8aCGnt2mTqvz0M9rw17y5PkUVeXFxwhVtalCsUL6UZuJA+fPF0fOcuuzae5D5q9Lv+1a8SH4OHEoi8cDhcIYbVZ7p/wRnn3sep57W2u9QfPPHb9OoU78Rzz/1MDde2ol7bu7GN5+NQ/Xoz0PS4cO8OOAxrrz+lmOScF6TV393YqFmG9FDRBH5G5dkFfhZRAJ/WeOBk4BJkYwpuxJ2JpCUlETZsuVSlZcpW5ZtM6b7FFXkNDyxNFMHXkShAvHs2X+Ibs9PZeGahJT557eoyph7zqJIwXxsSkjkkgHfsWXX/jTXVbJIAR7v1px3f1xC0pG0jsFiz2efTGDt2tU8M/g5v0Px1eYN65n8xcd0vrI7l1/bg5XL/mXEK26fXHRZNwA+Gv0WJUqW4oIuV/oZalTIq7870ZIwcyLS7THJN6toBHwD7AmYdxBYBXya3otFpDfQG+C1N97m5l69wxOlydSSDbs4/cEvKVGkAF1bV2f4ne244MnJLFq7E4BfFmzi9Ae/pGzxQvQ4uw7v33cWHR/7hk0796VaT9FC+fj40U5s2LGXfu/P8uGdRN6qlSt49ZWhvPveh+TPn3eazdOieoST6zbg+t53AVCzdj02rFvD5C8mcNFl3fh77iymfjuRoSPH+RypMTkT0WSrqk8BiMgqYLyqpl3VSf/1w4HhAPsPp9kMHTGlS5UmPj6e7du3pSrfsX075fJAB45Dh4+wYtNuAOat2E6LWuW48+KG3P7m/wGQeOAwKzbtZsWm3fy5dCt/vXo5N55dhyGf/JWyjqKF8vHZY+cAcMWgKRw4lBT5N+KD+X/NY2dCAldceklKWVJSEnNmz+KTCeOZ/udcChRI65bhsad02XLHNA1XPakGX3/qzj8umDeLhO3b6HHZuSnzjxxJ4r3hw5j4yVhGffJtROP1W1793YmBiq1vl/6MEZFCInIFcDLwtqruFJGTgQRVjfobW+QvUID6DRoyY/p0zj3vgpTy33+fztnnnJvBK2NTnAgF86c/fkScQMF8R+cXK5SPz/ueiwhc+sz37N2fd87Vduh4Ng0+b5Sq7Ml+j3HiSSdxc68+eaq2W69RU9avXZWqbMPaNZSvUAmACy69ijPan51q/lMP3UG7judz7sVdIxVm1MirvzvWjJxNIlILmAIUw9268WNgJ3Cb9/wWP+LKqutv7EHfRx6i0SmNadqsOR9P+IitW7Zw5dXd/A4trJ7u3oJv56xj3ba9FC+cn6va1qRdw4pcPugHihfOz71dTmHS7DVsSthHuRKF6HN+PaqULcpnv68EXKL96vHzKF4kP92GTKVoofwULeQSzI49Bzh0+Iifby/sipcoQfGg3seFCxemZMmS1Kpdx6eo/NH5yu48ckcPPn5/JG07nMuKZf/yzWfjuO6WOwAoVboMpUqXSfWa+Ph8lC5TlionVvchYv/lxd+dGMi1vt1B6mXge1xy3RlQ/hW56NKf8y+4kF07Exjx9pts3bqFWrXr8Ppbw6lcuYrfoYVVhVKFGXX3mVQoVZj/Eg+yYHUCXQd+z5S/NlC4QDz1q5Xiho61KVO8IDt2H2D28m2c+8QkFqx2HaianVyOVnXdZULzX7s81brPf3Iyvy7cFPH3ZPxRu15DHn3mRT4Y+RoT3htJ+QoVubbnbVxw6VV+hxa18urvTm4ngV3sI7ZRkR1Aa1VdIiK7gSaqukJEqgOLVbVwZuvw+5xttCvbLdccs0Tc1o9u8juEqLV6W6LfIUS1GuWL+h1C1CqUj7DVP1sM+ClHv/ezH+/ge93Yz6vD0zoxlWuuszXGGBMZsdCM7NdNLb4H7gt4riJSAngKd0mQMcYYA9hNLXLiPuAnEfkXKASMB2oBWwA7WWOMMSZFlOTLHPHr0p8NItIUuAZojqthDwc+VNV9Gb3WGGOMyW38GvWnoKruU9V3cOPZbgXq4obZM8YYY1LEQjNyRJOtiNQVkYVAoojMFZEGwB+4ZuXeuKblSyMZkzHGmOhmo/5k3QvARqAzsAA36MB3uOH2SgNvA49EOCZjjDFRzGq2WdcaeEBVvwFux13q87qqHlHVI8CrQL0Ix2SMMSaKhbtmKyLVROQnEVkkIgtF5H9eeX8RWS8i87zpwoDXPCoiy0TkXxE5L7NtRLqDVFlgA4Cq7haRvUBCwPwE3ADyxhhjTKQcBu5X1TkiUhyYLSI/ePOGquoLgQt7p0C7AQ2BysAUEamjqumOpuJHb+TgO4HYnaCMMcakK9xNwaq6EXeKM7kiuBjI6P6XXYBxqnoAWCkiy4DTgN/Te4EfyfYDETngPS4EjBCR5HvEFfQhHmOMMVEsp7k2cCx0z3BvyNa0lq0ONMN13m0D3CkiNwCzcLXfBFwinhHwsnVknJwjnmzHBD3/II1l3otEIMYYY3KHnNZsA8dCz2Q7xYBPgXtU9T8ReRMYgGuBHQC8CPTMTgyRHjy+RyS3Z4wxxoRCRPLjEu2HqvoZgKpuDpg/Avjae7oeqBbw8qpeWbr8ujeyMcYYE5JwX/ojbqFRuFHnXgoorxSwWFfcJavghoPtJiIFRaQGUBuYmdE2/Bz1xxhjjMlUBC6VbQNcD/wtIvO8sseAa7xbCyuwCugDoKoLRWQCsAjXk/mOjHoigyVbY4wxUS4CvZF/gzTH452UwWsGAgND3YYlW2OMMVEtSm4ClSN2ztYYY4wJM6vZGmOMiWrRcn/jnLBka4wxJqrFQK61ZGuMMSa6xcVAts12shWR/Kp66HgGY4wxxgSLgVwbWgcpEblbRC4PeD4K2OcNLVQ3bNEZY4wxMSDU3sh3A1sBRORM4CrgWmAe7l6RxhhjTFjEwuDxoTYjVwFWeo8vAT5W1Qki8jfwa1giM8YYY4C46MiXORJqzfY/4ATv8TnAj97jQ7hh8owxxpiwyEs12+9x487OAWoBk73yhhyt8RpjjDEmDaEm2ztw94A8EbhCVXd45c2Bj8IRWGaOqPqx2Vxj60c3+R1C1Crb+WW/Q4haS8fe7ncIUc1+dzISvhpklFROcySkZKuq/wF3pVH+5HGPyBhjjAkgYUzkkZJushWRMqGuJKCma4wxxhxXsdBBKqOa7TbcGH4ZEW+Z+OMWkTHGGBMgWjo55URGybZDxKIwxhhjYli6yVZVf45kIMYYY0xaYqBiG/q9kUWkAnA9cDLwuKpuE5E2wAZVtct/jDHGhEUsDEQQ6r2RWwD/At2Bm4ES3qxzcJcEGWOMMWEhkrMpGoR6B6kXgFdUtRlwIKD8O6DNcY/KGGOMiSGhNiO3wNVog20EKhy/cIwxxpjUYr03cqB9QOk0yusBW45fOMYYY0xqMZBrQ25G/hJ4UkQKes9VRKoDQ4BPwxGYMcYYA66DVE6maBBqsn0AKIMb07YI8BuwDNgJ9AtLZMYYYwzu7kk5maJBVu6N3FZEOuIGH4gD5qjqlHAGZ4wxxsSCkK+zBVDVqcDUMMVijDHGHCMWOkiF2oyMiFwqIr+IyDZv+lVEuoYzOGOMMSZOcjZFg1BvanE/MB53Y4uHvOkfYKyIPBC+8IwxxuR1IpKjKRqE2oz8AHCnqo4IKHtHRGYCT+NuemGMMcYcd1GSL3Mk1GRbDPgpjfKfvHlZJiL5gNOAE4ECgfNU9b3srNMYY4yJRqEm2y+AK4DBQeWXA19ldaMiUg+YCNTA9cxO8mI5hLsdpCVbY4wxQGx0kEo32YrIfQFPlwGPiEgH4HevrLU3vZSN7b4MzAaaApu8/0sCb2LX7RpjjAkQLZ2cciKjmu1dQc8TgDreFFh2E+68bVacCrRX1b0icgTIp6pzROQh4FWgcRbXZ4wxJkbFdM1WVWuEcbsCJHqPtwJVcD2d1wG1wrhdY4wxJhURqYY7fVkBUGC4qr4iImVwV+JUB1YBV6lqgrjs/wpwIS6X3aSqczLaRsjX2R5nC4Am3uOZwMMi0h54CtdkbYwxxgARuV3jYeB+VW2AOz16h4g0AB4BflTV2sCP3nOAC4Da3tQbdwo0QyHfQUpE6uA6SaXVe7hnqOvxDASKeo/7Ad/gejZvA67O4rqMMcbEsHAPJqCqG3FDxqKqu0VkMa7FtQtwlrfYGGAa8LBX/p6qKjBDREqJSCVvPWkKKdmKyEW40X3m4sa2/RM4GSgI/JqNN/ZdwOMVQH2vup7gBW+MMcYAkb3O1hvRrhnwB1AhIIFu4uj47VWAtQEvW+eVpZtsQ21Gfhp4SlVPx12acz2uDXsKLtNniYi8IyLFA8tUdQdQRETeyer6jDHGxK6c3kFKRHqLyKyAqXc62ymGq1je4w3Ak8KrCGa7Mhhqsq2LO0kM7lrYIqq6H5eE78nGdm8ECqdRXhi4IRvr892oEW/TrFE9Bg/MasfsvCGv7J8+Fzdh5hvXsfnT29n86e1Me+lqzj817b6Gr97ViX2T7+Wey1ukKq9RqSTjH7+ENeP6sPnT2/ng0Ys4oVSRSIQfcV98Mo5bul/OJR1P55KOp3PnLdcx4/9+SZm/Y/t2hjzdj6su7sSF7U/jkXtuZd2a1T5GHF3yyvcqp1R1uKq2DJiGBy8jIvlxifZDVf3MK94sIpW8+ZWALV75eqBawMuremXpCjXZ7gYKeY83crTHcD6gdIjrQETKiEhZ3Dnr0t7z5Kk8cDGwOdT1RYv5f83js08mULtOXb9DiUp5af+s37abfu/8xul3fkibu8cy7a+1THjiEhpVL5dqua5ta9OybkU2bNuTqrxIwXx8PfAyBLjgkU/oeP94CuSL49P+XWLilnXByp9QgV533MNbY8bzxuiPaNbiNJ546B6WL12CqvLEw/9j/do1PD3kZd5+bzwnVKzMg3f3Zt++xMxXHuPy0vdKJGdT5usXAUYBi1U18N4RX+Eqh3j/fxlQfoM4rYFdGZ2vhdCT7R9AW+/xN8CLIvIk8C5Hb3IRim24IwMFFuEu+0meNgEjgTeysD7f7d69m74PP0j/AQMpUaKE3+FEnby2f76esYLvZ61ixcZdLFu/k/5jprN73yFa1a+UssyJJxTnhT5ncdOQyRxKSkr1+tMbVqZ6hZL0Hvo9C1dtZ+Gq7dzy4nc0r12Bs5qcGOm3E3ZtzuxAqzPaUaXaiVQ7sTo333Y3RYoWYdGCv1i3djWLF8znfw/1pV7DU6h2Ug3ueagfBw/sZ+r3k/0O3Vd57XsVJ5KjKQRtcKdHO4rIPG+6EHfXxHNEZClwNkfvojgJWIG7emYEcHum7yHE93ofMMN73B/4HnerxmXALSGuA6AD0AlXs70C6BgwtQVOVNWBWVif757p/wRnn3sep57W2u9QolJe3j9xccKV7etQrFB+ZizeAEB8nDDm4QsZPO4P/l2745jXFMyfD1Vl/8HDKWX7DyVxRJUzGlaOWOx+SEpKYuoPk9mXmEjDU5pw6OBBAAoUOHrxQ1xcHPnzF2DBX3P9CjMq5LXvVbhrtqr6m6qKqjZW1abeNElVt6tqJ1Wtrapne32LUOcOVT1ZVU9R1VmZbSOk3shej+Hkx4nAbW4HSEFcr+SQqOrP3utqAGtye8/jzz6ZwNq1q3lm8HN+hxKV8ur+aVi9LNNe6kahAvnYs+8gVw+YyMJV2wF4/PrT2fbfPkZ8Mz/N1878ZyN79h9i0M1n0vcd19H/mZ5tyRcfR8UyRdN8TW63YtkS7up1PQcPHqRw4SI8NeRlataqw+HDhzihYiVGvTmM+x7tT+EiRfjko/fZumUzO7Zv8zts3+TF71VM30EqRPWAOUB8ZguKSPOgorLp7cDM7sQRDVatXMGrrwzl3fc+JH/+/H6HE3Xy8v5Zsi6BVnd8QMmiBenatjYj7j+P8x7+mLIlCnP92Q1pdecH6b522659dH/2a4bd2YneFzfhiCoTpv3LnKWbOZK7j03TVe2kGgx/72P27t3DL1N/YMjT/XjpjVHUOLk2Tw0eygsDn6Tree2Ii4+nxamtOO30tuTy4/Rsy8vfq9xOcvKhFZEmwBxVDSXZHsGdq83sEEXTW5/XXbs3wKtvvNWi5y1p9t6OiK+++Iwn+z1GfPzRUJOSkhAR4uLimP7n3FTNX3lNNO+fsp1fjuj2vnn2ctZs+Y91W3fz2LWtUyXNfPFxJCUdYVPCXmpdPzJ1nCUKcThJ2bX3ACs/7M2wz2Yz9NPZYY116dhMTz2F3YN39uKESpV5sO9TKWV79uzm8KFDlCpdhjt6Xkud+g3534N9Ix5bmWL+fqej+XtVJH/4qp93fb44R0dXr3at73vVOKc126zI8b2Wve7awwESD/l7aNuh49k0+LxRqrIn+z3GiSedxM29+uT5o07bP0fFCRTMH8/wr//i89+Wppo38ZnLmPDzP7wzecExr9v+334A2jepxgmlivD1jBXHLBOLjuiRlPO1yYoVc5flr1uzmiX/LKJHnzv9CM13efV7Zc3IWaCqMXVxXPESJSge1AuwcOHClCxZklq166Tzqrwjr+6fAT3a8u3MlazdupviRfJz9Vn1OLNxNbo++QVbd+1j6659qZY/lJTE5oRElq5PSCm7/pwGLFmbwJZdibSqV4kXbj2LVz+fk2qZWDHi9Zdp1aYdJ5xQkcTEvUz9fjJ/zZnFwBdfA+DnH7+nRKlSVKhYmZXLl/L6S0Noc2YHWrY6w+fI/ZFXv1exPsReWudZg2X7Ai8ROQXog+tg1VNVN4rIpcBqVc3bXQ1NrlWhdBHeefB8KpQpwq69B1mwchtdHv+cKXNCP9asU7UMT9/UljLFC7F68388N24mwz6P+m4M2bJj+zYG9X+MhO3bKFqsGDVPrsOgoW9waus2AGzftpU3X3mehB3bKVOuPOdecAnX9ezjc9TGZF2G52xDPM+a7jnWDNZ7Lu6i4Mm4IYrqq+oKEbkfaKeql2a2Dr+bkU3uFelztrlJNJyzjWZ+n7ONZuE8Z3vfV//k6Pf+pc71fK8bZ9aMHK4xbQcA96nqGyKyO6B8GnB/mLZpjDEmF4r5c7ZhPM/aCHcHjmA7gDJh2qYxxphcKBbO2fo1ePwO3HBEwZrjhioyxhhjgPDfQSoS/Eq2Y4HnRaQq7pxwPhFpD7wAvOdTTMYYY0xYRPI620D9gNHAalznq0W4xP8hkKvujWyMMSa8QhxMIKr5kmxV9RDQXUQexzUdxwFzVXVpxq80xhiT1/jVBHs8ZSnZikg53HWx81T1QHY2KCKFgYdwowbVxDUjrwA+FpEXVXVfRq83xhiTt8RAxTa0ZCsixXED616BS461gRUi8hawSVX7h7iefMBUXG32W9zYuAI0AJ4ALhCR9qp6OP21GGOMyUvyUjPyEFzv4ebAbwHlX+POsfYPcT29gVpAc1VdGDhDRBoBPwG9gDdDXJ8xxhgT9UJtCu8M3KOq83A122SLcU3BoboCGBicaAFUdQEwCLgyC+szxhgT4/LSpT+lge1plBcHkrKwvYa4ZuT0TMHd8MIYY4wB3E0tcjJFg1CT7Z+42m2y5NptH2B6FrZXGtiawfytQKksrM8YY0yMixPJ0RQNQj1n+xjwnYg09F5zn/f4NODMLGwvHsio89MRbxljjDEmZoSUbFV1uoicATwALAc6AXOA01X17yxsT4APRCS9y4YKZmFdxhhj8oAoqZzmSMjX2XpJ9cYcbm9MCMvY7RqNMcakiJbzrjkR6nW2GY7Eo6o7QlmPqvYIZTljjDEmmWQ4pHruEGrNdhupL/kJZudZjTHGhEWeqdkCHYKe5weaAbfhBhUwxhhjTDpC7SD1cxrFU0RkBXALbsg8Y4wx5rjLSzXb9Mwja5f+GGOMMVkiMdAdOdvJVkSKAfcAa49bNMYYY0yQPFOzFZHdpO4gJUARYC/QPQxxGWOMMUDeus72zqDnR3C3VvxDVROOb0jGGGNMbMk02Xpj0BYFvlDVDeEPyRhjjDkqWu5vnBOZDkTgDeT+PO5yH2OMMSai8tKoPzOAFuEMxBhjjElLLIxnG+o52xHACyJyIjAb1zEqharOOd6BGWOMMbEiw2QrIu/gLu9JvmnFS2kspvhwu0bN6OaRJgbuJBo+CRPv9TuEqFX61OC+kCbQ9pmv+h1CnhQX5l80L9ddDGxR1UZeWX+gF0fHYH9MVSd58x4FbgaSgLtV9bvMtpFZzfZG4BGgRnbegDHGGJNTEWgKHg28xrGjzg1V1RdSxyINgG5AQ6Ay7m6KdVQ1KaMNZJZsBUBVV2chaGOMMea4CXcnJ1X9RUSqh7h4F2Ccqh4AVorIMuA04PeMXhRKBylrsDXGGOObOJEcTSLSW0RmBUy9Q9z0nSIyX0TeEZHSXlkVUt85cZ1XlvF7CGFjm0QkKaMpxKCNMcaYiFPV4araMmAaHsLL3gROBpoCG4EXcxJDKL2RewM7c7IRY4wxJrv8uHxHVTcf3b6MAL72nq4HqgUsWtUry1AoyXaiqm7JSpDGGGPM8eLHHaREpJKqbvSedgUWeI+/AsaKyEu4DlK1gZmZrS+zZGvna40xxvgq3LlWRD4CzgLKicg64EngLBFpisuDq4A+AKq6UEQmAIuAw8AdmfVEhhB7IxtjjDF+CfVWh9mlqtekUTwqg+UHAgOzso0M34Oqxh3vJmQRKSAiT4nIEhHZb52tjDHGxLpwHzCkZQDuZhkv4obqexB4HdgO3O5DPMYYY6KYuMt3sj1FAz+S7VXArar6Nu5WV1+q6t24NvJzfIjHGGNMFJMcTtEg1IEIjqcKuBPLAHuAUt7jb4EhPsRjjDEmiuWJ8WzDYA2uuzTAMuA87/HpwD4f4jHGGGPCyo9k+znQyXv8CvCUiKzE3Qh6pA/xGGOMiWLWjJwNqvpowONPvGuazgCWqOrX6b/SGGNMXhQDrci+nLNNRVVnADP8jsMYY0x0ipYexTnhS7IVkQpAG+AEgpqyVfUNP2IyxhgTnfw433m8RTzZish1uHOzAiSQ+paQCliyNcYYE1P8qNkOBJ4DnlbVwz5s3xhjTC5izcjZUwIYbYnWGGNMKHJ/qvWnKfxD4CIftmuMMSYXioXbNfpRs70P+EJEOgF/A4cCZ6rq0z7EZIwxxoSNH8m2D3A+sA2oxbEdpCzZGmOMSWG9kbPnceB+VR3qw7aNMcbkMtHSFJwTfiTbeOArH7ZrjDEmF8r9qdaf2vm7QHcftmuMMSYXEsnZFA38qNkWAW4RkfOA+RzbQepuH2IyxhhjwsaPmm19YC5wEKgHnBIwNfIhnmzbunULT/R9mI5nnk7rFo25vMtFzP5zpt9hRYW33niVZqfUSzWdfVZbv8OKGrNn/cndd9zK2R3a0aRhXb78/DO/Q4qIPledyczxj7L51+fZ/OvzTBtzP+e3bZgyv2jhArz08JUs+3YAO35/ib8+f5y7undId31fvHYb++a+Rtezm0Yg+ugzasTbNGtUj8EDY7tfaRySoyka+DHqT/rfnFxk93//0fP6a2navAXDXn+b0qXLsG7dWkqXLet3aFGjevUajHj3vZTncXHxPkYTXRITE6lVuw6XdL6Ufo897Hc4EbN+SwL9hn3JsjVbiJM4rrukFRNe6s0Z3YewYOkGhtx/OR1b1aVnv/dYtX47bVvU4o3Hr2Hbzj189M2fqdZ1z/WdOHJE09lS7Jv/1zw++2QCtevU9TuUsIuWpuCciIUe1b4Y/e5IypUvz4Bnh9DolMZUqVqVVq1Pp2bNk/0OLWrE58tHuXLlU6YyZcr4HVLUaHdme+6+5z7OOe98RPLO1/DraX/z/f8tYsXabSxbs4X+r09kd+J+WjWuAUDrJjUY+81Mfpm1lDUbdzD265nM/HsVpzWqnmo9LRqcyB3XnkWfJz/w4V34b/fu3fR9+EH6DxhIiRIl/A4n7CSH/6JBxL/lIlJQRO4Vka9EZIaIzAycIh1Pdk2b+iONTmnMww/cS6f2Z9DtiksZN/YDVPPukXaw9evWck7Hdlx0ficefvA+1q1d63dIJorExQlXnteCYkUKMuOvlQBMn7eCC888haoVSgEu+TauU5Xvpy9OeV2xIgUZPegm7njmI7Ym7PEjdN890/8Jzj73PE49rbXfoUSEdZDKnhHAxcCXwCJS39Qi11i/bi0fj/+I7tffSI+be/HvP4t5btBAALpde53P0fmv0SlNeGrAIKrXqMmOHdsZOfxNbrr+Gj75YiKlSpX2Ozzjo4a1KjNtzP0UKpCPPfsOcPV9I1i4bAMA9w/5mNf6XcPSb5/h0KEkAO577mMm/7og5fWv9u3GD9MX8/3/LfIlfr999skE1q5dzTODn/M7FJMFfiTbzkAXVf05qy8Ukd5Ab4Bhr79Fz1t6H+/YQnbkiNKgYUPuuud+AOrVb8CaNauZMG6sJVugbbszUz1v3KQJF19wDhO//ILrb+zhU1QmGixZtZlW3QZRslhhup7djBFPX895vV5h0fKN3H5Ne1o3qcHl/3uLNRt30LZ5LQbd25XVG7bzw/TFXHPRqZxSpwptuufNRLNq5QpefWUo7773Ifnz5/c7nIiJlk5OOeFHst2Cu1VjlqnqcGA4wN6D/rbXlitfnpon10pVVqPmyXz04fs+RRTdihQpyskn12LNmtV+h2J8duhwEivWup+AuYvX0qLhidx1XQfuHfwxT9/Vme4PjWLSL64mu2DpBhrXrco9N3Tih+mL6XBaXerXrMi2/3sx1TrfH9yTP+avpFPP2L4x3fy/5rEzIYErLr0kpSwpKYk5s2fxyYTxTP9zLgUKFPAxwvCIlqbgnPAj2T4GPCsiN6lqgg/bPy6aNm3GqlUrU5WtWbWKSpUq+xRRdDtw4ACrVq6k5amt/A7FRJk4EQrmz0f+fPEUyJ+PpKTUx9FJSUeI835t+782kZff+zHV/Nmf9OXRoZ8zcdr8iMXslw4dz6bB56mvkHyy32OceNJJ3NyrT8zWdi3ZZs/3uMEItojIJo69qUVNH2LKsu433ESP669h5PC3OPf8C/h38WLGjX2fO+6+1+/QosJLLwzhzPYdqFSpMjt2bGfE22+wb18il3S51O/QokLi3r2sWbMGANUjbNy4gX8WL6ZkyZJUqhy7B2wD7u7Mt78uZO2mBIoXLcTVF7TkzJa16Xr3W+zeu59fZi1lwN2d2ZN4gDUbd9CuRS26X3wafV/5EoANW3exYeuuY9a7bnMCq9Zvj/TbibjiJUpQPKj3ceHChSlZsiS1atfxKSoTCj+S7XtAA+BlYDO5tINUw0an8OIrr/HaK0MZ+fYbVKxUidvuvJurul3rd2hRYfPmzTz68P3sTNhJ6TKlOaVxE8Z8OJ7Klav4HVpUWLhwAbf0uCHl+Zuvv8qbr79K5y5dGfDsYB8jC68KZUvwzsAbqVC2OLv27GfB0vV0ufNNpvzuehvf8Mg7PH1XF0Y/eyOlSxRhzcYdPP3GN7w5LstdPEwMiZbLd3JCIn2piojsBTqq6h85WY/f52yjXe7/aIZPXJztnfSUPvVOv0OIattnvup3CFGrSP7wNfb++M+2HP3ed6pXzvcvvR812zXAAR+2a4wxJheKhZqtH7euuRd4TkRqZbqkMcaYPM9uapE9HwMFgX9F5ABwOHCmqsb+vceMMcbkKX4kWzspZIwxJmThbkYWkXdwdzbcoqqNvLIywHigOrAKuEpVE0REgFeAC4FE4CZVnZPZNvwY9WdMpLdpjDEm94pAn8bRwGu4q2WSPQL8qKqDReQR7/nDwAVAbW9qBbzp/Z8hP2q2KUSkIpDqdiequsancIwxxkShcNdsVfUXEakeVNwFOMt7PAaYhku2XYD31F3KM0NESolIJVXdmNE2Ip5sRaQkMAy4iqBE67FBT40xxqTwqZNThYAEugmo4D2uAgQOYbbOK8sw2frRG/kFoAlwKbAfuBZ4EBfw1T7EY4wxJoaJSG8RmRUwZWkUG68Wm6Nrff1oRr4AuEZVfxWRJGC2qo4XkY242zh+4kNMxhhjolROK7aBg9hkwebk5mERqYQbRAdgPVAtYLmqXlmG/KjZlgKSh37ZBZT1Hv8OnOFDPMYYY6JYnEiOpmz6CrjRe3wjbgz25PIbxGkN7MrsfC34k2yXA8mDDSwGunldqS8DdvgQjzHGmCgmOZwyXb/IR7gKX10RWSciNwODgXNEZClwtvccYBKwAlgGjABuD+U9+NGMPBpojOvZNRj4GnftbRzwPx/iMcYYk4ep6jXpzOqUxrIK3JHVbfhxne3QgMdTRaQe0BJYqqp/RzoeY4wxUS5KbrmYExFLtiJSFLhEVcd5z98ACgUsclhE7lXVvZGKyRhjTPSzgQiypgfu2tpk1wMnAeW96XyyUTU3xhgT22JhIIJIJttuwMigsl6qeomqXoK7M8flEYzHGGNMLhDuDlKREMlkWwtYGPB8J5AU8HwWUD+C8RhjjDEREckOUiWBwslPVLVa0Px8QP4IxmOMMSY3iJbqaQ5Esma7Fjglg/lNSH2/SWOMMQbJ4b9oEMlk+w3QX0QKBc/weio/6S1jjDHGpIiFDlKRbEYehOuN/K+IvAYs8crrcfSmFoMiGI8xxphcIEryZY5ELNmq6hYROQN4C3fnqOT9p8D3wO2quiW91xtjjDG5VUTvIKWqq4ELRKQMrncywDJVtXsiG2OMSVsMVG39uDcyXnKd6ce2jTHG5C7R0skpJ3xJtsYYY0yooqWTU074McSeMcYYk6dYzdYYY0xUi4GKbe5NtvsPJWW+UB5WKH+83yGYXOiXzwb6HUJUO+WRb/0OIWotf/GC8K08BrJtrk22xhhj8gbrIGWMMcaEmXWQMsYYY0ymrGZrjDEmqsVAxdaSrTHGmCgXA9k24snWu1XjQKATcAJBTdmqWiLSMRljjIle1kEqe0YBzYDhwAbcQATGGGNMmmKhg5QfybYTcI6q/uHDto0xxpiI8yPZbgH2+LBdY4wxuVAMVGx9ufSnL/C0iBTzYdvGGGNyG8nhFAX8qNn2A6oDW0RkNXAocKaqNvYhJmOMMVHKOkhlzyc+bNMYY4zxTcSTrao+FeltGmOMyb2sN3IOiEhHoAHu0p+FqjrNr1iMMcZErxjItb7c1KIK8DnQAnedLUBlEZkFdFXVDem+2BhjTN4TA9nWj97Iw4AkoJaqVlPVakBtr2yYD/EYY4yJYpLDf9HAj2bkc4CzVHVlcoGqrhCRu4EffYjHGGOMCSu/ztmmdYtGu22jMcaYY8RCByk/mpF/BF4VkWrJBSJyIvAyVrM1xhgTJBL3tBCRVSLyt4jM8/oQISJlROQHEVnq/V86u+/Bj2R7N1AUWCEiq70bWyz3yu72IR5jjDHRLHJ3kOqgqk1VtaX3/BHgR1WtjasMPpLdt+BHM/J24DTgLKCeV7ZYVaf4EIsxxpgo52Mnpy64XAUwBpgGPJydFUU02YpIPLALaKKqPwA/RHL7xhhj8h4R6Q30DigarqrDgxZT4HsRUeBtb34FVd3ozd8EVMhuDBFNtqqa5DUbF4jkdo0xxuReOe0g5SXO4OQarK2qrheRE4AfROSfoHWol4izxY9ztgOAwSJSzodtG2OMyWUiccpWVdd7/2/B3XjpNGCziFQC8P7fkt334EeyfQBoC6wXkeUiMj9w8iEeY4wx0SzM2VZEiopI8eTHwLnAAuAr4EZvsRuBL7P7FmzUH2OMMXldBeBzce3V+YCxqvqtiPwJTBCRm4HVwFXZ3YCN+hOi994ZwbSpP7Bm9SoK5C9Aw1Mac+td93JyrdoAHD50iLffGMaM//uN9evWUrRYUZq3PI3b7rqXipUq+xy9P7Zu3cKrL7/Ib7/+QuLevVSpWo3H+j1Ji1NP8zu0qDD+ow8Z/e4otm3dysm1avPQI4/RvEXLzF8YQ44kJfHpByOYPnUyO3dsp1SZspzR4Xwuu74X8fHu5+m689P+vJx98RXcdOdDkQw3rG7tWJPzTqlAjROKcfDwEeat3skLk/5lyaY9KcsUKRDPgxfV4ZxGFSldND8bEvYx9ve1vPvLqpRlPrztNFrXKptq3V/P3cD/PvgrUm/luAt3b2RVXQE0SaN8O9DpeGzDj4EI2gOo6s9plKuq/hLpmEIxZ9ZMLrvyGuo3bASqjHjrNf53282M/eQrSpQsxf79+1nyz2JuvLk3tevWY++e3bz60vPcd2cf3hv/Ofny+TbAki92//cfPa+/lqbNWzDs9bcpXboM69atpXTZspm/OA/4dvIknhv8LI/1e5JmzVswftxYbu/Ti8+/+oZKlfPOwdnEj99jysRP6PPAk1SrfjJrVi7j7RefIl/+AnTtfjMAr42dlOo1K5cu5sUn76fVmWf7EXLYtKpVhg+mr+HvtbsAuPf82rzX5zTOe+5Xdu07BEDfzvU4o045Hhj7F2t37OO0mqUZeNUpJOw9yBezj47h8vHMdbww6d+U5/sPHYnsmznOYuEOUn5kgKHA02mUlwD640YDijovvzEi1fMnBgzi3DNbM3/eXNq270Cx4sV55c2RqZZ5qN+TdL+iC6tXruDk2nUiGa7vRr87knLlyzPg2SEpZVWqVvUxoujy/ph36dylK5df6VqlHu37ONN/+5UJ4z/if/fe73N0kbN00XyatW5L89btAChfsTLNW7dj+b8LUpYpVSZ1X8rZv/9CxSonUr9x84jGGm49hs9K9fz+sfOZN/AcWtQozdRFrl9O8+ql+WL2emYs3wHA57P3cWWrajQ5sVSqZLvvYBLbdh+MXPBhFgO51pcOUnWBtNozFnjzcoXEvYkcOXKE4iVKpLvM3j17ATJcJlZNm/ojjU5pzMMP3Eun9mfQ7YpLGTf2A1TtFtiHDh5k8aKFnN6mTary089ow1/z5voUlT/qNGzK4r9ms2HtKgDWr17BonmzaHJqmzSX378vkRk//0CHCy6NXJA+KVowH/FxklKrBZi1MoGODU6gUqlCADSvXooGlYvzyz9bU7324maV+PPpTkx+sC2PXlKXogXjIxr78SaSsyka+FGz3QdUAlYGlVcBcs2h2MsvDKJ23Xo0atw0zfmHDh3k1aHP0fbMszihQsXIBhcF1q9by8fjP6L79TfS4+Ze/PvPYp4bNBCAbtde53N0/krYmUBSUhJly6ausZUpW5ZtM6b7FJU/LrnqBvbv28vDva8mLi6OpKQkunTrwTmXXJHm8tN/+o7Dhw/R7uyLIhxp5D1+aX0Wrv+PuasSUsqe/mIRz1zRiN8e78ChJNc0/NTni/hp8dFkO3HuRtZPXcHmXfupU7E4D1xYh7qVSnDT8D8j/h7MUX4k2++AISLSWVUTwN3sGRjkzUtX4F1AXhz2Bjf27BXuWNP0yotDmD93Dm++8z7x8cceMR4+fJin+j3Cnt27eW7o6z5E6L8jR5QGDRty1z2uSbRe/QasWbOaCePG5vlka46a8fMP/DZlErc/PICqJ9Vk9fIlvP/WS5SvWJmzzu9yzPI/Tf6C5q3PpESpbN8PPld4rHM9WtYozdWvzeBIQGPQDW1Ponn1UvQaNZv1Ce6c7aOX1GP9jn388u82AMbNWJuy/JJNe1izPZHP7zmDhlVKsHD9f5F+K8dJlFRPc8CPZPsA8AuwKuC62sa4i4WvzuiFgXcB2b73sC/tka+8MJgp30/m1bffpUrVasfMP3z4ME8+9iDLly3l9eHvUrJUqcgHGQXKlS9PzZNrpSqrUfNkPvrwfZ8iih6lS5UmPj6e7du3pSrfsX075cqV9ykqf3w0chgXXnEdp591LgDVatRi25aNfDV+zDHJdvXyJaxcupiretzuR6gR07dzPS5uVonub85k7Y59KeUF88XxwIV1ueu9uSnncP/duJv6VUpwy1k1UpJtsL/X7eJw0hGqly+Sa5NttDQF50TEz9l695lsgku6873pftz9kjdk9Fq/DX1+ED98N4lX33qH6jVqHjP/8KFDPPHI/SxfuoTX3n6XsnnshzNQ06bNWLUq9ZmCNatWUSmPXgYVKH+BAtRv0JAZ01M3Gf/++3SaNG3mU1T+OHhgP3FxqX+G4uLiUT229+zUyZ9TvmJlGjWL3UvHHu9Sn0uaVea6N2eyYsveVPPyx8dRIF8cSUdS1zOOHFHiMshGdSsWJ198HFv+OxCWmCMhcoP+hI8v16OoaiIwItMFo8gLgwbw7aSJDH5xGMVLlGD7NneOpHCRIhQpUpTDhw/T9+H7WLxwAc+//DoipCxTrFhxChYq5Gf4Edf9hpvocf01jBz+FueefwH/Ll7MuLHvc8fd9/odWlS4/sYe9H3kIRqd0pimzZrz8YSP2LplC1de3c3v0CKqWat2TJzwHuUrVKbqSTVZtfxfJn8+lradLky13IH9+5k+9VsuvvJ6JBaqOWnof1kDLm1Rhdvenc2ufYcoV9zdQj7xQBKJB5PYc+AwM5Zt56GL6pJ4MIn1CftodXIZuraswpCv3WU+J5YtQufmlfl58RZ27D1E7QrFeLRzPRas28XslQkZbT6qxcKfXCLRO1RELgMmquoh73G6VPWzUNYZ6WbkM5o3TLO8Z+/bueXWO9i4YT2XX3xumsv07f8MF3XuGs7wjlEov/+9D3/9ZRqvvTKU1atWUrFSJa6+pjvdrvX/xzI+Ljq+ueM/+pDR74xi69Yt1KpdhwcffpQWLU/1NabkazwjZV/iXj55721mTZ/GfzsTKFWmLK3bn0vX7jdToEDBlOV+/n4io15+llfe/4rSZf1rMer2Wvg6sC1/8YI0y1/5binDvl8GQLniBXjwwrq0rVuOUkXysz5hHxP+WMfIaa4VqVKpQrx4bRPqVCxGkYL52LRzHz8t2sqw75el6tUcpvjD9sXauOtgjn7vK5Us4PuXPlLJ9ghQUVW3eI/To6oaUpbw65xtbhENyTZaRUuyjUaRTra5TTiTbW4XzmS7adehHP3eVyyZ3/cvfUSakVU1Lq3HxhhjTKZ8T5U5l7fuIWiMMSbXiYFc60+yFZEKQBvgBIJ6RKvqG37EZIwxxoSLHwMRXAeMxB2sJACBbfEKWLI1xhiTIhZ6I/tRsx0IPAc8raqHfdi+McaYXCTcQ+xFgh/JtgQw2hKtMcaYkOT+XOvLqD8fArF/F3FjjDHHhd1BKnvuA74QkU7A30CqK61VNa2xbo0xxphcy49k2wc4H9gG1OLYDlKWbI0xxqSwDlLZ8zhwv6oO9WHbxhhjchnrIJU98cBXPmzXGGNMLhQLNVs/Oki9C3T3YbvGGGOML/yo2RYBbhGR83Bj2QZ3kLrbh5iMMcaYsPEj2dYH5nqP6wXNs5F8jDHGpBILzcgRT7aq2kFESgK1vaJlqroz0nEYY4zJHWKhg1REz9mKyIkiMhHYDvzhTdtE5CsROTGSsRhjjMkdRHI2RYOI1WxFpAowAzgCPAEs8mY1BG4HfheRU1V1Q6RiMsYYYyIhks3ITwIrgbNVdV9A+RciMhT43lumTwRjMsYYE+WipHKaI5FMthcC3YMSLQCqmigi/YAPIhiPMcaY3CAGsm0kk215YHkG85d5yxhjjDEpYqGDVCST7RbcvZDXpTO/treMMcYYkyJaOjnlRCR7I08GnhGRgsEzRKQQMACYFMF4jDHGmIiIZM22PzALWCYirwH/eOUNcL2R8wFXRzAeY4wxuUAMVGwjl2xVdYOInAG8ATzL0f2nwHfAnaq6PlLxGGOMySUikG1F5HzgFdxgOSNVdfDxXH9E7yClqquAC0WkNKnvILUjknEYY4zJPcLdQUpE4oHXgXNw/Yr+FJGvVHVRxq8MnR/3RkZVE4CZfmzbGGOMCXIaruK3AkBExgFdOHrzpRzzJdkaY4wxoYpAb+QqwNqA5+uAVsdzA7k22ZYtmi+qzpmLSG9VHe53HNHI9k3Gomn/nFqjpN8hpBJN+wZg+YsX+B1CKtG2f8KlUL6ctSOLSG+gd0DR8EjvNz8Gj49VvTNfJM+yfZMx2z/ps32TMds/IVDV4araMmAKTrTrgWoBz6t6ZceNJVtjjDF53Z9AbRGpISIFgG7AV8dzA7m2GdkYY4w5HlT1sIjcibsMNR54R1UXHs9tWLI9fmL+vEkO2L7JmO2f9Nm+yZjtn+NEVScRxrsYiqqGa93GGGOMwc7ZGmOMMWFnyTYTIqIickUWlj/Le025cMZlYoeIVPc+My39jsVEt6z+HpnoYckWEJHmIpIkIv/ndyzRTERGe1/25GmbiHwtIvWysI7+IrIgnHFGStD+OCQiW0TkJxG5Q0Ty52C9ueKATUQqiMhQEVkqIvu99z9dRO4SkWJ+x+cXEakiIsNFZJ2IHBSR9SIyQkSqHofVVwImHof1mAizZOvcghsgoZGI1Pc7mCg3BfeFrwScCxQGPvc1In8l74/quP0xEXgK+FVEivoYV1iJSHVgDnA+8DjQHHfHnWeBTkBn34LzkYjUwI1u1gi4ETeG93VAQ9z9dqvnZP2quklVD2QztnwisTAybC6lqnl6wiWLncApwCjghaD5ClzhPa7uPb8W+A3Yjxsq8NyA5c/ylukE/AEk4r58zQOWKQt8hLsl2D5gIdDD730Rwr4aDXwdVHax934Le8+rAOOABG/6BqjtzbvJWzZwusmbdx8wH9iLu5h8JFDK7/ec1f3hlTcCDgJPec8LAEO8v3ci7pq+8wKWT/5ctQx4HDiN9pY7H/jV2687cJcp1PfpvU/G3d6uaDrzkztfZvh39T4Te4ALvO9SIu76xpLAFcBSYBfwfvJnzHvdNOBN4EVvX2wF/gcUxN1QfiewBrg+KK7BwL/e924V8BxQ6Djul0ne+ywSVF7EK/8mh/Gn/B55zysDHwLbvX03D+jgzesPLPD28XIgCSgGnIg7QN7tTZ8BVQPWmfy6bt7rdgNfAOX8/s7l5slqtu4LvVpV/8Z9oW8IoQnwOWAY0BT4AfhSRKoELTMIeAR3xL8d+DDgqLIQrlZwMe6I9xXgbRHplPO3EzkiUhw3BvHfqrpPRIoAP+EOQtoDpwMbgSnevPG4H5d/OVo7Hu+t7ghwD25/XIu7MfirEXszx5GqLgC+BS73it7F7Y9rcYl4DDBRRJqk8fK1Aa9riNtH//OeFwVexu2bs3BJaKJ3EX7EiEhZ4DzgdVXdm9Yy6v1qE9rftSBwP9Add5DaEvgUVzO8HLgU9125Peh13XGJoBUuib6MSwpLvHWMAUaKSKWA1+wFegL1vfV1A/qG9MYzISJlcAdEr6tqYuA87/kbwAXeqGfZjT9we0WBn3EHaJfiKgxPBy1WA7ffrwSa4A4CvwQqAB28qTLwRVCttzruu90V12LTDBgY0o4wafM72/s94Y4wH/AeC+5oN/DIMa2abd+A+XG4L8cz3vOzvGUCay5tvLKqGcQxDjeGou/7JIMYRwOHcTWRPd57WgM08ub3xNVEJOA18biDjau85/2BBSFs63zgABDn9/vOZH8cU7P15g3G1TROxiWcE4PmfwG8EfS5ahn0GcqwJoFLvklA2wi/71ZefF2DytcFfDbeCuXvytHWjroBy7zgva9yAWWp9rX3vf094LngaodfBZTlxyWXKzJ4L7fiRnsJ234JmN/Vm39aduMn9e9RL1yyTvNz4n3XDgEVAsrO8fZt9YCymt5n9OyA1+0HSgYs0/d47ae8OuXpmq2I1ALaAmMh5Wj8Q+DmTF76e/IDVT2Cay5uELTM/IDHG7z/T/C2Gy8ifUVkvohsF5E9wGW45p1o9wuuRt8U96PxI/C9iFQDWuCOpHeLyB7vfe0CSuOSTrpEpKOI/OB1Kklu2ioAVAzXGwkzwf0wNvceL0reJ95+uYhM9skxKxQ5WUTGishyEfkP2Iw72IuWz0073OdiJq71JtS/6wFV/Tfg+WZgk6puCyo7IWh7Kd8x77u7Bfg7oOwQrsk95XUicoWI/CYim7y/w1D8239Zjj9IM2B+0H4Ktk5VNwc8rw9sUDe2ePJ2VuB+owJ/w1ar6q6A5xsyiMOEIK/fQeoWXM1rTUALigCISDVVXZveC0NwKOBxcpNa8sHNA7hms//hvlx7cB1LcsOHOVFVlyU/EZFbcAm1N+79zcM1zQXbkd4KReQk3LndEcATuJpwc9x57Yg2kR5HDYAVuH2iwKmk/kyAO2+YFV/jao99cOf/DuPG24z0PlqGe0+peqGr6koAEUn0/g/173o4aP3KsftKObZDZ1rLpPs6EWmNa0F6CrgXd160M64mfTwk75cGpN1psIE3P/n7k6X4synNZv50aMDj4x1Hnpdnd56I5MOdE3qUozW1prjzGvOBHhm8vHXAegRXw1uchc23BSaq6vuqOg/XCaFOFl4fTRTXBFUEdx66FrBNVZcFTcnJ9iDuACdQS9yP772q+ruqLsGdR8qVRKQRrrn0E2Au7gCuYhr7JL1RRQ56/6fsJ+88aT3gWVWdoqqLgeL4cMCsqtuB74E7M7nEJ9r+rm2A9ao6QFX/VNWlwEnHa+XefvkOuN3ro5DCe34HMDngu5BTc4HGWbxEbDFQObBXtIjUxP1djttA6eZYeTbZ4prxygEjVHVB4IQ7+u2RQTf527zmqLq4Tg0n4XoWhmoJ0ElE2nrXqL6Ga37NDQqKSEVvqo/r7FIMd8nLh7jmvi9FpL24ETTOFJEXRaS29/pVwEnetc3lRKQg7jxvHHCP95prcJ1qcoPk/VFZRJqIyH2483GzcT3bl+D2y2jvM1NTRFqKyAMiclk661yNO4i5SETKewktAdgG9BKRWiLSHniLY2uFkXI77m82W0SuEZEGIlLH+9s1wZ0XjLa/6xKgioh09/4OtwHXHOdt3Ik7AJriNaFXE5GzcB0pxZt/vIzFNT1/KSLtvPfUWUQ6ZPCaKbjKxIfe57Al7vM5B5h6HGMzQfJysr0Z+Mk7Gg32Ma7TyjnpvPYR3CUNf+FqMF1VdV0Wtv0M7rzWZNw50L24D3xucDauh/FG3LnqU4ErVXWauh6XZ+KaTz/GXcoxBnfONsF7/ae4yyN+xHUIuUZV5+Oa1O/DHV3fgmtqzw2S98ca3HvqjOtgcqYe7anbA9cj+TncPvkat59Wp7VCr8b7JK7352bgNa9vwNVAY9xlGa/jrm/N1jWXOeWd52uG63U9AFfLmoP7G74B3BNtf1dVnQg8jztAno/7fj9xnLexHFejX4i7umEFLikuBk5Nbmo/Ttvai+vlvg53sLsA10SuGbxGgS64795P3rQJuNSbZ8LEBiLIAq/pZSXuSzPL53CMMcbkEnm5ZmuMMcZEhCVbY4wxJsysGdkYY4wJM6vZGmOMMWFmydYYY4wJM0u2xhhjTJhZsjXG4910QgOe3+TdP9ePWL4WkdFh3oaKyBU5XIdv+8iY3MSSrYlqIjLaSwoqIodEZIWIvCCRGZh9PG5ElJCIyCoRichNG0TkLG+fZOVWfcYYn+T1gQhM7jAFuB435Fg73ADkRYHbghf07nmddDzuhqOq+8j6YAHGGHMMq9ma3OCAqm5S1bWqOhZ3a8tLAUSkv4gs8Jozl+NuX1hUREqKyHAR2SIiu0XkZ+8+sClE5AYRWS0iiSLyNW5A7cD5xzSRisiFIvKHiOwTNzziRBEpJCLTcPfIfj65Jh7wmjO87SeKyHoReVNESgTML+LV4PeIyGYReSynO0xEThWR70Vkm4j8J25YudPTWLSiiHzjxbZaRK4LWk8VERknIgne9E3Afa7T2m41EflSRHZ46/xHRNIaBcqYPMWSrcmN9uFquclqANcCV+Jugn8AN7RbFeBi3D18fwGmikglABFphRuQfDhutKeJwNMZbVREzge+wt1UvgXQAfgZ9z26DHeP2qeBSt6EiJyCGyHnKy+2y7ztvROw6hdw9+m9HOjkxXtmyHsjbcVx9+ZthxuVah4wSdzoQYGe8mJritsX7yUflIgbqeYn3EDi7YHTcfeBniJBo9oEeAM3AlQHoCFu4IGdOXwvxuR+fo9eb5NNGU24hPh1wPPTcKPfjPee98eNvVkhYJmOuDGCCwetax7wkPd4LPBD0PyRePdq957fBOwJeP5/wLgMYl0FPBBU9h4wKqisKe5m8SfgRkw6AHQPmF8Ml6BGZ7Cts7x1lAtxPwouUV4XUKa4Ua8Cl5sCfOA97okbuUcC5sfjxqW9Kp19NB940u/PjU02Rdtk52xNbnC+15ybD1ej/RK4K2D+OlXdHPC8Ba52tVVSj5JYCDjZe1wfV5sN9DtuNKj0NMMl/6xoAdQSkasDypKDOhlIxI35+nvyTFXdIyJ/Z3E7qYjICbjReDrgmsfjgcLAiUGL/p7G84sCYq8B7A7aj0U4uh+DvQK85bUC/Ah8rqqzs/k2jIkZlmxNbvAL0BtXg92gqoeC5u8Neh6HG5quXRrr+u/4h5ehOFyNeWga89YDdcK03TG4JHsvrsZ9AJf8CmRhHXG41oC0zrmmOQC6qo4Ske+AC3HDD04XkUGq2j8L2zUm5liyNblBoqouy8Lyc3CJ5oi6cVfTshhoHVQW/DzYXNw51RHpzD+Iq0EGx9Iwvfi9Tl2HvG2v8MqKAo2A5ZnEk5G2wN2q+o23zgp455GDtCb1+ePWuH2THPs1wDZV3RnqhtWN7TwcGC4iD+PGtO2fxfiNiSmWbE0smoI7v/qliDyEG7C9InA+MEVVfwWG4WpdjwKf4M6Bds1kvQOBiSKyDHfOV4BzgbdVNRFXg2wnIh/gelBvA4YAM0TkLeBtYDdQD7hEVft4TcajgCEishXYgBvQPDhpp6eRiOwMKpsPLAGuE5E/cJdJPYc7GAh2mYj8CUwDrsAdTLTy5n2IG+z9SxF5AlgDVMMNPv6Wqi4NXpmIvAJM9rZfArfPF4X4XoyJWdYb2cQcVVVcM+ZUXC30X2ACUBeXzFDVGbjzs7fhktNlZFL7UtVJuIR8Aa6W+zPunOgRb5EncMloObDVe818XM/i6t7yfwGDcM3cyR7A9fr93Pt/Aa7pPBQ/ebEETkVwnZuKAbOBcbja66o0Xt8f1wt6Pm5f9FDVP73YE73YVwAf4w5axgClgYR04okDXsUl2B+893ljiO/FmJhlQ+wZY4wxYWY1W2OMMSbMLNkaY4wxYWbJ1hhjjAkzS7bGGGNMmFmyNcYYY8LMkq0xxhgTZpZsjTHGmDCzZGuMMcaEmSVbY4wxJsz+H/b3hrtoCTkHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a normalized confusion matrix\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "heatmap=sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=VOC_NAMES,\n",
    "            yticklabels=VOC_NAMES,\n",
    "            annot_kws={\"fontsize\": 14})\n",
    "\n",
    "# Customize the x-axis and y-axis tick label font size\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), fontsize=14)  # Increase the x-axis tick label font size here\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), fontsize=14)  # Increase the y-axis tick label font size here\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(f'Misclassification Analysis ({MODEL_PRINT_NAME}-{EMBED_SIZE})', fontsize=14)\n",
    "plt.xlabel('Predicted Labels', fontsize=14)\n",
    "plt.ylabel('True Labels' , fontsize=14)\n",
    "plt.savefig(f'figures/missclassification_{MODEL_PRINT_NAME}_{EMBED_SIZE}.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e67902",
   "metadata": {},
   "source": [
    "# SAVE AND LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03de52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = f\"./checkpoints/{MODEL_PRINT_NAME}_{EMBED_SIZE}\"\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "# Save tokenizer\n",
    "tok.save_pretrained(ckpt_dir)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "523610fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(ckpt_dir)\n",
    "\n",
    "# Load model\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283aed9",
   "metadata": {},
   "source": [
    "# REFERENCES:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ce50c",
   "metadata": {},
   "source": [
    "https://grabngoinfo.com/transfer-learning-for-text-classification-using-pytorch/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
